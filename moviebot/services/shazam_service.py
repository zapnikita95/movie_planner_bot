"""
–°–µ—Ä–≤–∏—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ñ–∏–ª—å–º–æ–≤ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é (–®–∞–∑–∞–º)
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç TMDB –¥–∞—Ç–∞—Å–µ—Ç (–æ—Ñ—Ñ–ª–∞–π–Ω), semantic search, –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫ –∏ whisper
"""
import os
import logging
import pandas as pd
import numpy as np
import faiss
import json
from pathlib import Path
from sentence_transformers import SentenceTransformer
from transformers import pipeline
import torch
import gc
from tqdm import tqdm
from datetime import datetime
# Whisper –∑–∞–º–µ–Ω—ë–Ω –Ω–∞ faster-whisper –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

# –í –Ω–∞—á–∞–ª–µ —Ñ–∞–π–ª–∞ (–ø–æ—Å–ª–µ –≤—Å–µ—Ö –∏–º–ø–æ—Ä—Ç–æ–≤)
import threading

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞
_index_lock = threading.Lock()
# –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ embeddings
_model_lock = threading.Lock()
# –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ Whisper
_whisper_lock = threading.Lock()

# –û—Ç–∫–ª—é—á–∞–µ–º –Ω–µ–Ω—É–∂–Ω—ã–π –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ segmentation fault
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'

logger = logging.getLogger(__name__)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –∫—ç—à–∏ –º–æ–¥–µ–ª–µ–π
_model = None
_translator = None
_whisper = None
_index = None
_movies_df = None
_top_actors_set = None  # –ú–Ω–æ–∂–µ—Å—Ç–≤–æ —Ç–æ–ø-500 –∞–∫—Ç—ë—Ä–æ–≤
_top_directors_set = None  # –ú–Ω–æ–∂–µ—Å—Ç–≤–æ —Ç–æ–ø-100 —Ä–µ–∂–∏—Å—Å—ë—Ä–æ–≤

# –ü—É—Ç–∏ ‚Äî –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞, –Ω–∞ Railway —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–∞–∫ –∂–µ
CACHE_DIR = Path('cache')
CACHE_DIR.mkdir(parents=True, exist_ok=True)

# DATA_DIR –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤, —Ç–æ–ø-—Å–ø–∏—Å–∫–æ–≤ –∏ –∫—ç—à–∞ Whisper
# –ù–∞ Railway volume –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω –≤ app/data, —á—Ç–æ–±—ã –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω—è–ª–∏—Å—å –º–µ–∂–¥—É –¥–µ–ø–ª–æ—è–º–∏
DATA_DIR = Path('data/shazam')
DATA_DIR.mkdir(parents=True, exist_ok=True)

# –ö—ç—à –¥–ª—è huggingface –º–æ–¥–µ–ª–µ–π
os.environ['HF_HOME'] = str(CACHE_DIR / 'huggingface')
os.environ['TRANSFORMERS_CACHE'] = str(CACHE_DIR / 'huggingface' / 'transformers')
os.environ['SENTENCE_TRANSFORMERS_HOME'] = str(CACHE_DIR / 'huggingface' / 'sentence_transformers')

TMDB_CSV_PATH = CACHE_DIR / 'tmdb_movies.csv'  # 'cache/tmdb_movies.csv'
INDEX_PATH = DATA_DIR / 'tmdb_index.faiss'     # 'data/shazam/tmdb_index.faiss'
DATA_PATH = DATA_DIR / 'tmdb_movies_processed.csv'  # 'data/shazam/tmdb_movies_processed.csv'
TOP_ACTORS_PATH = DATA_DIR / 'top_actors.txt'  # –¢–æ–ø-500 –∞–∫—Ç—ë—Ä–æ–≤
TOP_DIRECTORS_PATH = DATA_DIR / 'top_directors.txt'  # –¢–æ–ø-100 —Ä–µ–∂–∏—Å—Å—ë—Ä–æ–≤

MIN_VOTE_COUNT = 500
MAX_MOVIES = 20000

# –ü–∞—Ä–∞–º–µ—Ç—Ä fuzziness –¥–ª—è –ø–æ–∏—Å–∫–∞ (0-100)
# 0 = —Å—Ç—Ä–æ–≥–∏–π –ø–æ–∏—Å–∫ (—Ç–æ–ª—å–∫–æ —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è)
# 50 = —Å—Ä–µ–¥–Ω–∏–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
# 100 = –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π (—Å–∞–º—ã–µ –æ—Ç–¥–∞–ª—ë–Ω–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è)
FUZZINESS_LEVEL = int(os.getenv('FUZZINESS_LEVEL', '50'))
FUZZINESS_LEVEL = max(0, min(100, FUZZINESS_LEVEL))  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 0-100

# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–∫—Ç—ë—Ä–æ–≤ –∏ —Ä–µ–∂–∏—Å—Å—ë—Ä–æ–≤ –≤ —Ç–æ–ø-—Å–ø–∏—Å–∫–∞—Ö (–º–æ–∂–Ω–æ –º–µ–Ω—è—Ç—å –±–µ–∑ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏)
TOP_ACTORS_COUNT = int(os.getenv('TOP_ACTORS_COUNT', '500'))
TOP_DIRECTORS_COUNT = int(os.getenv('TOP_DIRECTORS_COUNT', '100'))


def init_shazam_index():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω–¥–µ–∫—Å–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    logger.info("–ó–∞–ø—É—Å–∫ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∏–Ω–¥–µ–∫—Å–∞ —à–∞–∑–∞–º–∞ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è...")
    try:
        # –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–ª–æ–∫–∏—Ä–æ–≤–∫—É –∑–¥–µ—Å—å - get_index_and_movies() —É–∂–µ –∑–∞—â–∏—â–µ–Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–æ–π
        get_index_and_movies()  # –≠—Ç–æ –≤—ã–∑–æ–≤–µ—Ç build_tmdb_index() –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        logger.info("–ò–Ω–¥–µ–∫—Å —à–∞–∑–∞–º–∞ —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∏–Ω–¥–µ–∫—Å–∞ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ: {e}", exc_info=True)


def get_model():
    global _model
    # –î–≤–æ–π–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å –±–ª–æ–∫–∏—Ä–æ–≤–∫–æ–π –¥–ª—è thread-safety
    if _model is None:
        with _model_lock:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—â–µ —Ä–∞–∑ –≤–Ω—É—Ç—Ä–∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
            if _model is None:
                logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ embeddings...")
                # –ü–æ–∑–≤–æ–ª—è–µ–º –≤—ã–±—Ä–∞—Ç—å –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –Ω–∞ Railway
                model_name = os.getenv('EMBEDDINGS_MODEL', 'BAAI/bge-large-en-v1.5')
                # –ï—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω USE_FAST_EMBEDDINGS=1, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –ª–µ–≥–∫—É—é –º–æ–¥–µ–ª—å
                if os.getenv('USE_FAST_EMBEDDINGS', '0').strip().lower() in ('1', 'true', 'yes', 'on'):
                    model_name = 'BAAI/bge-base-en-v1.5'
                    logger.info("‚ö†Ô∏è USE_FAST_EMBEDDINGS=1 ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –ª–µ–≥–∫—É—é –º–æ–¥–µ–ª—å –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è")
                _model = SentenceTransformer(model_name)
                logger.info(f"–ú–æ–¥–µ–ª—å embeddings –∑–∞–≥—Ä—É–∂–µ–Ω–∞ ({model_name} ‚Äî –ª—É—á—à–∞—è –¥–ª—è retrieval –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º)")
    return _model


def get_translator():
    global _translator
    if _translator is None:
        logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ —Ç—Ä–∞–Ω—Å–ª—è—Ç–æ—Ä–∞ ru‚Üíen...")
        try:
            torch.set_num_threads(1)
            torch.set_grad_enabled(False)
            _translator = pipeline(
                "translation",
                model="facebook/nllb-200-distilled-600M",
                src_lang="rus_Cyrl",
                tgt_lang="eng_Latn",
                device=-1,
                torch_dtype=torch.float32
            )
            test = _translator("—Ç–µ—Å—Ç–æ–≤–∞—è —Ñ—Ä–∞–∑–∞", max_length=512)
            logger.info(f"–¢—Ä–∞–Ω—Å–ª—è—Ç–æ—Ä –≥–æ—Ç–æ–≤ (—Ç–µ—Å—Ç: '—Ç–µ—Å—Ç–æ–≤–∞—è —Ñ—Ä–∞–∑–∞' ‚Üí '{test[0]['translation_text']}')")
            logger.info("–¢—Ä–∞–Ω—Å–ª—è—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω (nllb-200-distilled-600M ‚Äî –ª—É—á—à–µ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤)")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å–ª—è—Ç–æ—Ä–∞: {e}", exc_info=True)
            _translator = False
    return _translator


def get_whisper():
    global _whisper
    # –î–≤–æ–π–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å –±–ª–æ–∫–∏—Ä–æ–≤–∫–æ–π –¥–ª—è thread-safety (–∫–∞–∫ –≤ get_model)
    if _whisper is None:
        with _whisper_lock:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—â–µ —Ä–∞–∑ –≤–Ω—É—Ç—Ä–∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
            if _whisper is None:
                logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ faster-whisper...")
                try:
                    # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º faster_whisper —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ (–ª–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞)
                    try:
                        from faster_whisper import WhisperModel
                    except ImportError as import_error:
                        logger.error(f"‚ùå faster-whisper –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {import_error}")
                        logger.error("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —á–µ—Ä–µ–∑: pip install faster-whisper")
                        _whisper = False
                        return _whisper
                    
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å "small" - —Ö–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –∫–∞—á–µ—Å—Ç–≤–æ–º –∏ —Ä–∞–∑–º–µ—Ä–æ–º
                    # –ú–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è WHISPER_MODEL (base, small, medium, large-v2, large-v3)
                    model_size = os.getenv('WHISPER_MODEL', 'small')
                    device = "cpu"  # –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å "cuda" –µ—Å–ª–∏ –µ—Å—Ç—å GPU
                    compute_type = "int8"  # int8 –¥–ª—è CPU, float16 –¥–ª—è GPU
                    
                    # –ö—ç—à Whisper —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ DATA_DIR (data/shazam/whisper) - —ç—Ç–æ volume –Ω–∞ Railway
                    # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–µ —Å–∫–∞—á–∏–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –∫–∞–∂–¥–æ–º –ø–µ—Ä–µ–¥–µ–ø–ª–æ–µ
                    whisper_cache = DATA_DIR / 'whisper'
                    whisper_cache.mkdir(parents=True, exist_ok=True)
                    
                    logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ faster-whisper: {model_size} (device={device}, compute_type={compute_type})...")
                    logger.info(f"–ö—ç—à –º–æ–¥–µ–ª–µ–π Whisper: {whisper_cache} (volume –Ω–∞ Railway: app/data/shazam/whisper)")
                    model = WhisperModel(model_size, device=device, compute_type=compute_type, download_root=str(whisper_cache))
                    
                    class WhisperWrapper:
                        def __init__(self, model):
                            self.model = model
                            
                        def __call__(self, audio_path):
                            # faster-whisper –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä, –Ω—É–∂–Ω–æ —Å–æ–±—Ä–∞—Ç—å –≤—Å–µ —Å–µ–≥–º–µ–Ω—Ç—ã
                            segments, info = self.model.transcribe(str(audio_path), language="ru", beam_size=5)
                            text_parts = []
                            for segment in segments:
                                text_parts.append(segment.text)
                            full_text = " ".join(text_parts).strip()
                            return {"text": full_text}
                    
                    _whisper = WhisperWrapper(model)
                    logger.info(f"‚úÖ faster-whisper —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω (–º–æ–¥–µ–ª—å: {model_size})")
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ whisper: {e}", exc_info=True)
                    _whisper = False
    return _whisper


def _clean_russian_fillers(text):
    """–£–¥–∞–ª—è–µ—Ç —Ä—É—Å—Å–∫–∏–µ —Å–ª–æ–≤–∞-–ø–∞—Ä–∞–∑–∏—Ç—ã –∏ –º–µ–∂–¥–æ–º–µ—Ç–∏—è –∏–∑ —Ç–µ–∫—Å—Ç–∞ –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ–≤–æ–¥–æ–º"""
    import re
    
    # –ë–æ–ª—å—à–æ–π —Å–ø–∏—Å–æ–∫ —Ä—É—Å—Å–∫–∏—Ö —Å–ª–æ–≤-–ø–∞—Ä–∞–∑–∏—Ç–æ–≤, –º–µ–∂–¥–æ–º–µ—Ç–∏–π –∏ —Å–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç —É—Ö—É–¥—à–∏—Ç—å –ø–æ–∏—Å–∫
    russian_fillers = [
        # –ú–µ–∂–¥–æ–º–µ—Ç–∏—è –∏ –∑–≤—É–∫–∏
        '—ç', '—ç–º', '—ç–º–º', '–º–º', '–º', '–∞', '–∞—Ö', '–æ—Ö', '—É—Ö', '–æ', '–æ–æ', '–∞–∞', '—É—É',
        '—Ö–º', '—Ö–º—ã–∫', '—Ö–º—ã–∫–∞–Ω—å–µ', '—Ö–º-—Ö–º', '—Ö–º—ã–∫-—Ö–º—ã–∫', '—Ö–º-–º', '—Ö–º—ã–∫-–º—ã–∫',
        '–Ω—É', '–Ω—É—É', '–Ω—É-–Ω—É', '–Ω—É—É—É', '–Ω—É-—É', '–Ω—É-—É-—É',
        '—ç—ç', '—ç—ç—ç', '—ç—ç—ç—ç', '—ç–º–º–º', '—ç–º-—ç–º', '—ç-—ç', '—ç-—ç-—ç',
        '–∞–π', '–æ–π', '—ç–π', '—ç–π-—ç–π', '–∞–π-–∞–π', '–æ–π-–æ–π',
        '–∞–π–π', '–æ–π–π', '—ç—ç—ç—ç', '–º–º–º–º',
        
        # –°–ª–æ–≤–∞-–ø–∞—Ä–∞–∑–∏—Ç—ã (—Å–∞–º—ã–µ —á–∞—Å—Ç—ã–µ)
        '—Ç–∏–ø–∞', '—Ç–∏–ø–∞ —Ç–æ–≥–æ', '—Ç–∏–ø–∞ –∫–∞–∫', '—Ç–∏–ø–∞ –∫–∞–∫ –±—ã', '—Ç–∏–ø–∞ –Ω—É', '—Ç–∏–ø–∞ —Ç–∞–∫',
        '–∫–∞–∫ –±—ã', '–∫–∞–∫–±—ã', '–∫–∞–∫-–±—ã', '–∫–∞–∫ –±—É–¥—Ç–æ –±—ã', '–∫–∞–∫ –±—É–¥—Ç–æ', '–∫–∞–∫-–±—É–¥—Ç–æ',
        '–∫–∞–∫-—Ç–æ', '–∫–∞–∫—Ç–æ', '–∫–∞–∫ —Ç–æ', '–∫–∞–∫-—Ç–æ —Ç–∞–∫', '–∫–∞–∫ —Ç–æ —Ç–∞–∫',
        '—á—Ç–æ-—Ç–æ', '—á—Ç–æ—Ç–æ', '—á—Ç–æ —Ç–æ', '—á—Ç–æ-—Ç–æ —Ç–∏–ø–∞', '—á—Ç–æ-—Ç–æ —Ç–∏–ø–∞ —Ç–æ–≥–æ', '—á—Ç–æ-—Ç–æ –≤—Ä–æ–¥–µ',
        '–≤–æ—Ç', '–≤–æ—Ç —ç—Ç–æ', '–≤–æ—Ç —Ç–∞–∫', '–≤–æ—Ç —Ç–∏–ø–∞', '–≤–æ—Ç –∫–∞–∫-—Ç–æ', '–≤–æ—Ç —Ç–∞–∫ –≤–æ—Ç', '–≤–æ—Ç –∫–∞–∫ –±—ã',
        '–≤–æ–æ–±—â–µ', '–≤–æ–æ–±—â–µ-—Ç–æ', '–≤–æ–æ–±—â–µ —Ç–æ', '–≤–æ–æ–±—â–µ –≥–æ–≤–æ—Ä—è',
        '–≤ –æ–±—â–µ–º', '–≤–æ–±—â–µ–º', '–≤ –æ–±—â–µ–º-—Ç–æ', '–≤ –æ–±—â–µ–º —Ç–æ',
        '–∫–æ—Ä–æ—á–µ', '–∫–æ—Ä–æ—á–µ –≥–æ–≤–æ—Ä—è', '–∫–æ—Ä–æ—á–µ –≥–æ–≤–æ—Ä—è —Ç–∏–ø–∞', '–∫–æ—Ä–æ—á–µ —Ç–∞–∫',
        '–∑–Ω–∞—á–∏—Ç', '–∑–Ω–∞–µ—à—å', '–∑–Ω–∞–µ—à—å –ª–∏', '–∑–Ω–∞–µ—à—å —Ç–∏–ø–∞',
        '—Ç–æ –µ—Å—Ç—å', '—Ç–æ–µ—Å—Ç—å', '—Ç–æ –µ—Å—Ç—å —Ç–∏–ø–∞', '—Ç–æ –µ—Å—Ç—å —Ç–æ –µ—Å—Ç—å',
        '—Ç–∞–∫ —Å–∫–∞–∑–∞—Ç—å', '—Ç–∞–∫—Å–∫–∞–∑–∞—Ç—å', '—Ç–∞–∫ —Å–∫–∞–∑–∞—Ç—å —Ç–∏–ø–∞', '—Ç–∞–∫ —Å–∫–∞–∑–∞—Ç—å –∫–∞–∫ –±—ã',
        '–≤ –ø—Ä–∏–Ω—Ü–∏–ø–µ', '–≤–ø—Ä–∏–Ω—Ü–∏–ø–µ', '–≤ –ø—Ä–∏–Ω—Ü–∏–ø–µ –∫–∞–∫ –±—ã',
        '–Ω–∞–ø—Ä–∏–º–µ—Ä', '–Ω–∞–ø—Ä–∏–º–µ—Ä —Ç–∏–ø–∞', '–Ω–∞–ø—Ä–∏–º–µ—Ä –∫–∞–∫ –±—ã',
        '–¥–∞–≤–∞–π', '–¥–∞–≤–∞–π —Ç–∞–∫', '–¥–∞–≤–∞–π –∫–∞–∫ –±—ã', '–¥–∞–≤–∞–π —Ç–∏–ø–∞',
        '–≤—Ä–æ–¥–µ', '–≤—Ä–æ–¥–µ –±—ã', '–≤—Ä–æ–¥–µ –∫–∞–∫', '–≤—Ä–æ–¥–µ –∫–∞–∫ –±—ã', '–≤—Ä–æ–¥–µ —Ç–∏–ø–∞',
        '–∫–∞–∫ –≥–æ–≤–æ—Ä–∏—Ç—Å—è', '–∫–∞–∫–≥–æ–≤–æ—Ä–∏—Ç—Å—è', '–∫–∞–∫ –≥–æ–≤–æ—Ä–∏—Ç—Å—è —Ç–∏–ø–∞',
        '–º–æ–∂–Ω–æ —Å–∫–∞–∑–∞—Ç—å', '–º–æ–∂–Ω–æ—Å–∫–∞–∑–∞—Ç—å', '–º–æ–∂–Ω–æ —Å–∫–∞–∑–∞—Ç—å —Ç–∏–ø–∞',
        '–ø–æ —Å—É—Ç–∏', '–ø–æ—Å—É—Ç–∏', '–ø–æ —Å—É—Ç–∏ –¥–µ–ª–∞', '–ø–æ —Å—É—Ç–∏ –¥–µ–ª–∞ –∫–∞–∫ –±—ã',
        '–≤ —Å—É—â–Ω–æ—Å—Ç–∏', '–≤—Å—É—â–Ω–æ—Å—Ç–∏', '–≤ —Å—É—â–Ω–æ—Å—Ç–∏ –∫–∞–∫ –±—ã',
        '–Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ', '–Ω–∞—Å–∞–º–æ–º–¥–µ–ª–µ', '–Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ –∫–∞–∫ –±—ã', '–Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ —Ç–∏–ø–∞',
        '–≤ –∏—Ç–æ–≥–µ', '–≤–∏—Ç–æ–≥–µ', '–≤ –∏—Ç–æ–≥–µ –∫–∞–∫ –±—ã',
        '–≤ –∫–æ–Ω—Ü–µ –∫–æ–Ω—Ü–æ–≤', '–≤–∫–æ–Ω—Ü–µ–∫–æ–Ω—Ü–æ–≤', '–≤ –∫–æ–Ω—Ü–µ –∫–æ–Ω—Ü–æ–≤ –∫–∞–∫ –±—ã',
        '—Ç–∞–∫–æ–π', '—Ç–∞–∫–∞—è', '—Ç–∞–∫–æ–µ', '—Ç–∞–∫–∏–µ', '—Ç–∞–∫–æ–π —Ç–∏–ø–∞', '—Ç–∞–∫–æ–π –∫–∞–∫ –±—ã',
        '—Ç–∞–º', '—Ç—É—Ç', '–∑–¥–µ—Å—å', '—Ç—É—Ç–∞',
        '—ç—Ç–æ', '—ç—Ç–æ —Å–∞–º–æ–µ', '—ç—Ç–æ —Å–∞–º–æ–µ —Ç–∏–ø–∞', '—ç—Ç–æ –∫–∞–∫ –±—ã', '—ç—Ç–æ —Ç–∏–ø–∞',
        '–≤–æ—Ç —ç—Ç–æ', '–≤–æ—Ç —ç—Ç–æ –¥–∞', '–≤–æ—Ç —ç—Ç–æ —Ç–∏–ø–∞',
        '–±–ª–∏–Ω', '–±–ª–∏–Ω –∫–∞–∫ –±—ã', '–±–ª–∏–Ω —Ç–∏–ø–∞',
        '—á–µ—Ä—Ç', '—á–µ—Ä—Ç –ø–æ–±–µ—Ä–∏', '—á–µ—Ä—Ç –∑–Ω–∞–µ—Ç', '—á–µ—Ä—Ç –≤–æ–∑—å–º–∏',
        
        # –°–ª–æ–≤–∞-—Å–≤—è–∑–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –Ω–µ—Å—É—Ç —Å–º—ã—Å–ª–æ–≤–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏
        '—Ç–∞–∫', '—Ç–∞–∞–∫', '—Ç–∞–∞–∞–∫', '—Ç–∞–∫-—Ç–∞–∫', '—Ç–∞–∫ —Ç–∞–∫',
        '–¥–∞', '–¥–∞–∞', '–¥–∞–∞–∞', '–¥–∞-–¥–∞', '–¥–∞ –¥–∞',
        '–Ω–µ—Ç', '–Ω–µ-–∞', '–Ω–µ–∞', '–Ω–µ-–Ω–µ',
        '–ª–∞–¥–Ω–æ', '–ª–∞–¥–Ω–æ —É–∂', '–ª–∞–¥–Ω–æ-–ª–∞–¥–Ω–æ', '–ª–∞–¥–Ω–æ —Ç–∞–∫',
        '–æ–∫–µ–π', '–æ–∫', '–æ–æ–æ–∫', '–æ–∫–µ–π —Ç–∏–ø–∞', '–æ–∫ –∫–∞–∫ –±—ã',
        '–∞–≥–∞', '–∞–≥–∞-–∞–≥–∞', '–∞–≥–∞ –ø–æ–Ω—è–ª', '–∞–≥–∞ –∫–∞–∫ –±—ã',
        '—É–≥—É', '—É–≥—É-—É–≥—É', '—É–≥—É –∫–∞–∫ –±—ã',
        '–º–¥–∞', '–º–¥–∞-–∞', '–º–¥–∞ –∫–∞–∫ –±—ã',
        '–Ω—É –ª–∞–¥–Ω–æ', '–Ω—É –ª–∞–¥–Ω–æ —É–∂', '–Ω—É –ª–∞–¥–Ω–æ —Ç–∞–∫',
        
        # –í–≤–æ–¥–Ω—ã–µ —Å–ª–æ–≤–∞
        '—Å–ª—É—à–∞–π', '—Å–ª—É—à–∞–π –∫–∞–∫ –±—ã', '—Å–ª—É—à–∞–π —Ç–∏–ø–∞',
        '–ø–æ–Ω–∏–º–∞–µ—à—å', '–ø–æ–Ω–∏–º–∞–µ—à—å –ª–∏', '–ø–æ–Ω–∏–º–∞–µ—à—å —Ç–∏–ø–∞',
        '–≤–∏–¥–∏—à—å', '–≤–∏–¥–∏—à—å –ª–∏', '–≤–∏–¥–∏—à—å —Ç–∏–ø–∞',
        '–∑–Ω–∞–µ—à—å', '–∑–Ω–∞–µ—à—å –ª–∏', '–∑–Ω–∞–µ—à—å —Ç–∏–ø–∞',
        '—Å–ª—ã—à–∏—à—å', '—Å–ª—ã—à–∏—à—å –ª–∏', '—Å–ª—ã—à–∏—à—å —Ç–∏–ø–∞',
        '–ø—Ä–µ–¥—Å—Ç–∞–≤—å', '–ø—Ä–µ–¥—Å—Ç–∞–≤—å —Å–µ–±–µ', '–ø—Ä–µ–¥—Å—Ç–∞–≤—å –∫–∞–∫ –±—ã',
        '–ø–æ–Ω—è–ª', '–ø–æ–Ω—è–ª –¥–∞', '–ø–æ–Ω—è–ª —Ç–∏–ø–∞',
        
        # –ü–æ–≤—Ç–æ—Ä—ã –∏ –∑–∞–ø–æ–ª–Ω–∏—Ç–µ–ª–∏ –ø–∞—É–∑
        '–Ω—É —Ç–∏–ø–∞', '–Ω—É —Ç–∏–ø–∞ –∫–∞–∫', '–Ω—É —Ç–∏–ø–∞ —Ç–∞–∫',
        '—Ç–∏–ø–∞ –Ω—É', '—Ç–∏–ø–∞ –Ω—É –∫–∞–∫', '—Ç–∏–ø–∞ –Ω—É —Ç–∞–∫',
        '–∫–∞–∫ –±—ã –Ω—É', '–∫–∞–∫ –±—ã –Ω—É —Ç–∏–ø–∞', '–∫–∞–∫ –±—ã –Ω—É —Ç–∞–∫',
        '—ç—Ç–æ –∫–∞–∫ –±—ã', '—ç—Ç–æ –∫–∞–∫ –±—ã —Ç–∏–ø–∞', '—ç—Ç–æ –∫–∞–∫ –±—ã —Ç–∞–∫',
        '–≤–æ—Ç –∫–∞–∫ –±—ã', '–≤–æ—Ç –∫–∞–∫ –±—ã —Ç–∏–ø–∞',
        '—Ç–∞–∫ –≤–æ—Ç', '—Ç–∞–∫ –≤–æ—Ç —Ç–∏–ø–∞', '—Ç–∞–∫ –≤–æ—Ç –∫–∞–∫ –±—ã',
        '–∑–Ω–∞—á–∏—Ç —Ç–∞–∫', '–∑–Ω–∞—á–∏—Ç —Ç–∞–∫ —Ç–∏–ø–∞',
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞-–ø–∞—Ä–∞–∑–∏—Ç—ã
        '–∫—Å—Ç–∞—Ç–∏', '–∫—Å—Ç–∞—Ç–∏ –≥–æ–≤–æ—Ä—è', '–∫—Å—Ç–∞—Ç–∏ —Ç–∏–ø–∞',
        '—Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ', '—Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ –≥–æ–≤–æ—Ä—è', '—Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ —Ç–∏–ø–∞',
        '–≤–ø—Ä–æ—á–µ–º', '–≤–ø—Ä–æ—á–µ–º –≥–æ–≤–æ—Ä—è',
        '–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏', '–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –≥–æ–≤–æ—Ä—è',
        '–∫–æ–Ω–µ—á–Ω–æ', '–∫–æ–Ω–µ—á–Ω–æ –∂–µ', '–∫–æ–Ω–µ—á–Ω–æ —Ç–∏–ø–∞',
        '—Ä–∞–∑—É–º–µ–µ—Ç—Å—è', '—Ä–∞–∑—É–º–µ–µ—Ç—Å—è –∂–µ',
        '–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ', '–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ –∂–µ',
        '—Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ', '—Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ –≥–æ–≤–æ—Ä—è',
        '–≤ –ø—Ä–∏–Ω—Ü–∏–ø–µ', '–≤–ø—Ä–∏–Ω—Ü–∏–ø–µ',
        '–≤ –æ–±—â–µ–º-—Ç–æ', '–≤–æ–±—â–µ–º-—Ç–æ',
        '—Ç–∞–∫ –≤–æ—Ç', '—Ç–∞–∫ –≤–æ—Ç –∫–∞–∫ –±—ã',
        '–Ω—É –∏', '–Ω—É –∏ —Ç–∞–∫', '–Ω—É –∏ —Ç–∏–ø–∞',
        '–≤–æ—Ç –∏', '–≤–æ—Ç –∏ —Ç–∞–∫', '–≤–æ—Ç –∏ —Ç–∏–ø–∞',
        '—Ç–æ –µ—Å—Ç—å', '—Ç–æ–µ—Å—Ç—å',
        '—Ç–æ –µ—Å—Ç—å –∫–∞–∫ –±—ã', '—Ç–æ –µ—Å—Ç—å —Ç–∏–ø–∞',
        
        # –ú–µ–∂–¥–æ–º–µ—Ç–∏—è –∏ –∑–≤—É–∫–∏ (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ)
        '—Ö–º-—Ö–º', '—ç-—ç-—ç', '–º-–º-–º', '–∞-–∞-–∞', '–æ-–æ-–æ',
        '–∞–π-—è–π-—è–π', '–æ–π-—ë–π-—ë–π',
        '–±—Ä—Ä', '—Ç—Ä—Ä', '–ø—Ñ—Ñ', '—Ç—å—Ñ—É', '—É—Ñ',
    ]
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç (—É–±–∏—Ä–∞–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é, –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É)
    text_lower = text.lower()
    words = re.findall(r'\b\w+\b', text_lower)
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–ø–∏—Å–æ–∫ –≤ –º–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
    russian_fillers_set = set(russian_fillers)
    
    # –£–¥–∞–ª—è–µ–º —Å–ª–æ–≤–∞-–ø–∞—Ä–∞–∑–∏—Ç—ã
    cleaned_words = [w for w in words if w not in russian_fillers_set]
    
    # –£–¥–∞–ª—è–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –º–µ–∂–¥–æ–º–µ—Ç–∏—è (1-2 —Å–∏–º–≤–æ–ª–∞), –∫—Ä–æ–º–µ –≤–∞–∂–Ω—ã—Ö —Å–ª–æ–≤
    important_short_words = {'–¥–∞', '–Ω–µ—Ç', '–Ω–µ', '–æ–Ω', '–æ–Ω–∞', '–æ–Ω–∏', '–º—ã', '–≤—ã', '—è', '—Ç—ã'}
    cleaned_words = [w for w in cleaned_words if len(w) > 2 or w in important_short_words]
    
    # –¢–∞–∫–∂–µ —É–¥–∞–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ –ø–æ–¥—Ä—è–¥ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–Ω—É –Ω—É –Ω—É")
    final_words = []
    prev_word = None
    for word in cleaned_words:
        if word != prev_word or len(word) > 3:  # –†–∞–∑—Ä–µ—à–∞–µ–º –ø–æ–≤—Ç–æ—Ä—ã —Ç–æ–ª—å–∫–æ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Å–ª–æ–≤
            final_words.append(word)
        prev_word = word
    
    # –°–æ–±–∏—Ä–∞–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ç–µ–∫—Å—Ç
    cleaned_text = ' '.join(final_words)
    
    # –ï—Å–ª–∏ –≤–µ—Å—å —Ç–µ–∫—Å—Ç —É–¥–∞–ª–µ–Ω –∏–ª–∏ –æ—Å—Ç–∞–ª–æ—Å—å –æ—á–µ–Ω—å –º–∞–ª–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª
    if not cleaned_text.strip() or len(cleaned_text.strip().split()) < 2:
        return text
    
    return cleaned_text


def translate_to_english(text):
    translator = get_translator()
    if not translator or translator is False:
        return text
    
    russian_chars = set('–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è')
    if any(c.lower() in russian_chars for c in text):
        try:
            # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç —Ä—É—Å—Å–∫–∏—Ö —Å–ª–æ–≤-–ø–∞—Ä–∞–∑–∏—Ç–æ–≤ –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ–≤–æ–¥–æ–º
            cleaned_text = _clean_russian_fillers(text)
            if cleaned_text != text:
                logger.info(f"[TRANSLATE] –û—á–∏—â–µ–Ω —Ç–µ–∫—Å—Ç –æ—Ç —Å–ª–æ–≤-–ø–∞—Ä–∞–∑–∏—Ç–æ–≤: '{text[:100]}...' ‚Üí '{cleaned_text[:100]}...'")
            
            result = translator(cleaned_text, max_length=512)
            translated = result[0]['translation_text']
            
            # –§–∏–∫—Å –¥–ª—è "–í–µ–ª–∏–∫–∞—è –¥–µ–ø—Ä–µ—Å—Å–∏—è"
            if "great depression" in translated.lower():
                translated = translated.replace("great depression", "Great Depression")
                translated = translated.replace("Great depression", "Great Depression")
                translated = translated.replace("great Depression", "Great Depression")
            
            return translated
        except Exception:
            return text
    return text


def transcribe_voice(audio_path):
    """Whisper ‚Äî —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏"""
    logger.info(f"[TRANSCRIBE] –§–∞–π–ª: {audio_path}")
    
    whisper_model = get_whisper()
    if not whisper_model:
        logger.error("Whisper –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª—Å—è")
        return None
        
    try:
        result = whisper_model(audio_path)
        text = result.get("text", "").strip()
        if text:
            logger.info(f"[WHISPER] –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: {text[:120]}...")
            return text
        logger.warning("[WHISPER] –ü—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
    except Exception as e:
        logger.error(f"Whisper –æ—à–∏–±–∫–∞: {e}", exc_info=True)
    
    return None


def convert_ogg_to_wav(ogg_path, wav_path, sample_rate=16000):
    """–û—Å—Ç–∞–≤–ª—è–µ–º –ø—Ä–æ—Å—Ç—É—é –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—é —á–µ—Ä–µ–∑ pydub (–µ—Å–ª–∏ –µ—â—ë –∏—Å–ø–æ–ª—å–∑—É–µ—à—å)"""
    try:
        from pydub import AudioSegment
        audio = AudioSegment.from_ogg(ogg_path)
        audio = audio.set_frame_rate(sample_rate).set_channels(1)
        audio.export(wav_path, format="wav")
        return True
    except Exception as e:
        logger.error(f"–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è OGG‚ÜíWAV –ø—Ä–æ–≤–∞–ª–∏–ª–∞—Å—å: {e}")
        return False


def parse_json_list(json_str, key='name', top_n=10):
    if pd.isna(json_str) or json_str == '[]':
        return ''
    try:
        items = json.loads(json_str)
        names = [item[key] for item in items[:top_n] if key in item]
        return ', '.join(names)
    except:
        return ''

def _create_top_lists_from_dataframe(df):
    """–°–æ–∑–¥–∞—ë—Ç —Ç–æ–ø-—Å–ø–∏—Å–∫–∏ –∞–∫—Ç—ë—Ä–æ–≤ –∏ —Ä–µ–∂–∏—Å—Å—ë—Ä–æ–≤ –∏–∑ DataFrame"""
    logger.info("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö –∞–∫—Ç—ë—Ä–æ–≤ –∏ —Ä–µ–∂–∏—Å—Å—ë—Ä–æ–≤ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —Ç–æ–ø-—Å–ø–∏—Å–∫–æ–≤...")
    from collections import Counter
    
    all_actors = []
    all_directors = []
    
    for idx, row in df.iterrows():
        # –ê–∫—Ç—ë—Ä—ã - –ø—Ä–æ–±—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ø–æ—Å–æ–±–æ–≤ –ø–∞—Ä—Å–∏–Ω–≥–∞
        actors_found = False
        
        # –ü–æ–ø—ã—Ç–∫–∞ 1: –∏–∑ cast (JSON)
        if pd.notna(row.get('cast')):
            cast_value = row['cast']
            try:
                # –ü—Ä–æ–±—É–µ–º –ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ JSON
                cast_json = json.loads(cast_value) if isinstance(cast_value, str) else cast_value
                if isinstance(cast_json, list):
                    for actor_item in cast_json:
                        if isinstance(actor_item, dict) and 'name' in actor_item:
                            actor_name = str(actor_item['name']).strip().lower()
                            if actor_name and actor_name != 'nan':
                                all_actors.append(actor_name)
                                actors_found = True
            except (json.JSONDecodeError, TypeError, AttributeError):
                # –ü–æ–ø—ã—Ç–∫–∞ 2: –µ—Å–ª–∏ cast –Ω–µ JSON, –ø–∞—Ä—Å–∏–º –∫–∞–∫ —Å—Ç—Ä–æ–∫—É —Å –∑–∞–ø—è—Ç—ã–º–∏
                try:
                    cast_str = str(cast_value).strip()
                    if cast_str and cast_str != 'nan' and not cast_str.startswith('['):
                        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∑–∞–ø—è—Ç—ã–º –∏ –¥–æ–±–∞–≤–ª—è–µ–º –∫–∞–∂–¥–æ–≥–æ –∞–∫—Ç—ë—Ä–∞
                        for actor in cast_str.split(','):
                            actor_name = actor.strip().lower()
                            if actor_name and actor_name != 'nan':
                                all_actors.append(actor_name)
                                actors_found = True
                except (TypeError, AttributeError):
                    pass
        
        # –ü–æ–ø—ã—Ç–∫–∞ 3: –∏–∑ actors_str (–µ—Å–ª–∏ cast –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª)
        if not actors_found and pd.notna(row.get('actors_str')):
            try:
                actors_str = str(row['actors_str']).strip()
                if actors_str and actors_str != 'nan':
                    # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∑–∞–ø—è—Ç—ã–º –∏ –¥–æ–±–∞–≤–ª—è–µ–º –∫–∞–∂–¥–æ–≥–æ –∞–∫—Ç—ë—Ä–∞
                    for actor in actors_str.split(','):
                        actor_name = actor.strip().lower()
                        if actor_name and actor_name != 'nan':
                            all_actors.append(actor_name)
            except (TypeError, AttributeError):
                pass
        
        # –†–µ–∂–∏—Å—Å—ë—Ä—ã
        if pd.notna(row.get('director')) and str(row['director']).strip():
            director = str(row['director']).strip().lower()
            if director and director != 'nan':
                all_directors.append(director)
    
    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—É –ø–æ—è–≤–ª–µ–Ω–∏—è
    actor_counts = Counter(all_actors)
    director_counts = Counter(all_directors)
    
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∞–∫—Ç—ë—Ä–æ–≤: {len(actor_counts)}, —Ä–µ–∂–∏—Å—Å—ë—Ä–æ–≤: {len(director_counts)}")
    
    # –¢–æ–ø-N –∞–∫—Ç—ë—Ä–æ–≤ –∏ —Ä–µ–∂–∏—Å—Å—ë—Ä–æ–≤ (–∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è)
    top_actors = [actor for actor, count in actor_counts.most_common(TOP_ACTORS_COUNT)]
    top_directors = [director for director, count in director_counts.most_common(TOP_DIRECTORS_COUNT)]
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª—ã
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    try:
        with open(TOP_ACTORS_PATH, 'w', encoding='utf-8') as f:
            f.write('\n'.join(top_actors))
        logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω—ë–Ω —Ç–æ–ø-{TOP_ACTORS_COUNT} –∞–∫—Ç—ë—Ä–æ–≤: {len(top_actors)} –∏–º—ë–Ω (—Ñ–∞–π–ª: {TOP_ACTORS_PATH})")
        if top_actors:
            logger.info(f"   –ü—Ä–∏–º–µ—Ä—ã —Ç–æ–ø-5 –∞–∫—Ç—ë—Ä–æ–≤: {top_actors[:5]}")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–æ–ø-–∞–∫—Ç—ë—Ä–æ–≤: {e}", exc_info=True)
    
    try:
        with open(TOP_DIRECTORS_PATH, 'w', encoding='utf-8') as f:
            f.write('\n'.join(top_directors))
        logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω—ë–Ω —Ç–æ–ø-{TOP_DIRECTORS_COUNT} —Ä–µ–∂–∏—Å—Å—ë—Ä–æ–≤: {len(top_directors)} –∏–º—ë–Ω (—Ñ–∞–π–ª: {TOP_DIRECTORS_PATH})")
        if top_directors:
            logger.info(f"   –ü—Ä–∏–º–µ—Ä—ã —Ç–æ–ø-5 —Ä–µ–∂–∏—Å—Å—ë—Ä–æ–≤: {top_directors[:5]}")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–æ–ø-—Ä–µ–∂–∏—Å—Å—ë—Ä–æ–≤: {e}", exc_info=True)

def build_tmdb_index():
    global _index, _movies_df

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–π –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∏
    force_rebuild = os.getenv('FORCE_REBUILD_INDEX', '0').strip().lower() in ('1', 'true', 'yes', 'on')
    if force_rebuild:
        logger.warning("‚ö†Ô∏è FORCE_REBUILD_INDEX=1 - –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∞ –∏–Ω–¥–µ–∫—Å–∞!")
        # –£–¥–∞–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å –∏ –¥–∞–Ω–Ω—ã–µ
        try:
            if INDEX_PATH.exists():
                INDEX_PATH.unlink()
                logger.info("–£–¥–∞–ª–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å –¥–ª—è –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∏")
            if DATA_PATH.exists():
                DATA_PATH.unlink()
                logger.info("–£–¥–∞–ª–µ–Ω—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∏")
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ —Å—Ç–∞—Ä–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞: {e}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∏–Ω–¥–µ–∫—Å - –µ—Å–ª–∏ –¥–∞, –∑–∞–≥—Ä—É–∂–∞–µ–º –µ–≥–æ –≤–º–µ—Å—Ç–æ –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∏
    if not force_rebuild and INDEX_PATH.exists() and DATA_PATH.exists():
        logger.info(f"–ò–Ω–¥–µ–∫—Å —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç ({INDEX_PATH}), –∑–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ —Ñ–∞–π–ª–∞...")
        try:
            _index = faiss.read_index(str(INDEX_PATH))
            _movies_df = pd.read_csv(DATA_PATH)
            
            # –ö–†–ò–¢–ò–ß–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –∏–Ω–¥–µ–∫—Å–∞ –∏ —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏
            model = get_model()
            expected_dim = model.get_sentence_embedding_dimension()
            actual_dim = _index.d
            
            if expected_dim != actual_dim:
                logger.warning(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∏–Ω–¥–µ–∫—Å–∞ ({actual_dim}) –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é –º–æ–¥–µ–ª–∏ ({expected_dim})!")
                logger.warning(f"–ò–Ω–¥–µ–∫—Å –±—ã–ª –ø–æ—Å—Ç—Ä–æ–µ–Ω —Å –¥—Ä—É–≥–æ–π –º–æ–¥–µ–ª—å—é. –ü–µ—Ä–µ—Å–æ–±–∏—Ä–∞–µ–º –∏–Ω–¥–µ–∫—Å...")
                _index = None
                _movies_df = None
                # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–π –∏–Ω–¥–µ–∫—Å, —á—Ç–æ–±—ã –ø–µ—Ä–µ—Å–æ–±—Ä–∞—Ç—å
                try:
                    INDEX_PATH.unlink()
                    DATA_PATH.unlink()
                    logger.info("–°—Ç–∞—Ä—ã–π –∏–Ω–¥–µ–∫—Å —É–¥–∞–ª–µ–Ω –¥–ª—è –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∏")
                except Exception as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å —Å—Ç–∞—Ä—ã–π –∏–Ω–¥–µ–∫—Å: {e}")
            else:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ actors_str –∏ director_str –≤ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º DataFrame
                has_actors = 'actors_str' in _movies_df.columns
                has_director = 'director_str' in _movies_df.columns
                if not has_actors or not has_director:
                    logger.warning(f"–ò–Ω–¥–µ–∫—Å –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç actors_str –∏–ª–∏ director_str (has_actors={has_actors}, has_director={has_director})")
                    logger.warning("–î–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ keyword-–º–∞—Ç—á–∏–Ω–≥–∞ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–µ—Ä–µ—Å–æ–±—Ä–∞—Ç—å –∏–Ω–¥–µ–∫—Å —Å FORCE_REBUILD_INDEX=1")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–æ–ø-—Å–ø–∏—Å–∫–æ–≤ –∞–∫—Ç—ë—Ä–æ–≤ –∏ —Ä–µ–∂–∏—Å—Å—ë—Ä–æ–≤
                if not TOP_ACTORS_PATH.exists() or not TOP_DIRECTORS_PATH.exists():
                    logger.warning("‚ö†Ô∏è –¢–æ–ø-—Å–ø–∏—Å–∫–∏ –∞–∫—Ç—ë—Ä–æ–≤/—Ä–µ–∂–∏—Å—Å—ë—Ä–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
                    logger.warning("‚ö†Ô∏è –ü—ã—Ç–∞–µ–º—Å—è —Å–æ–∑–¥–∞—Ç—å —Ç–æ–ø-—Å–ø–∏—Å–∫–∏ –∏–∑ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
                    # –ü—ã—Ç–∞–µ–º—Å—è —Å–æ–∑–¥–∞—Ç—å —Ç–æ–ø-—Å–ø–∏—Å–∫–∏ –∏–∑ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ DataFrame
                    try:
                        _create_top_lists_from_dataframe(_movies_df)
                        logger.info("‚úÖ –¢–æ–ø-—Å–ø–∏—Å–∫–∏ —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω—ã –∏–∑ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö!")
                    except Exception as e:
                        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ç–æ–ø-—Å–ø–∏—Å–∫–æ–≤: {e}", exc_info=True)
                        logger.warning("‚ö†Ô∏è –î–ª—è —Ä–∞–±–æ—Ç—ã –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ –∞–∫—Ç—ë—Ä–∞–º/—Ä–µ–∂–∏—Å—Å—ë—Ä–∞–º –Ω—É–∂–Ω–æ –ø–µ—Ä–µ—Å–æ–±—Ä–∞—Ç—å –∏–Ω–¥–µ–∫—Å —Å FORCE_REBUILD_INDEX=1")
                
                logger.info(f"–ò–Ω–¥–µ–∫—Å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ —Ñ–∞–π–ª–∞, —Ñ–∏–ª—å–º–æ–≤: {len(_movies_df)}, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {actual_dim}")
                return _index, _movies_df
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∏–Ω–¥–µ–∫—Å–∞: {e}, –ø–µ—Ä–µ—Å–æ–±–∏—Ä–∞–µ–º...", exc_info=True)
    
    # –ò–Ω–¥–µ–∫—Å –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª—Å—è - –ø–µ—Ä–µ—Å–æ–±–∏—Ä–∞–µ–º
    logger.info("–ù–∞—á–∏–Ω–∞–µ–º –ø–µ—Ä–µ—Å–±–æ—Ä–∫—É –∏–Ω–¥–µ–∫—Å–∞ TMDB...")
    
    # === –°–ö–ê–ß–ò–í–ê–ù–ò–ï –ò –ü–û–ò–°–ö CSV –§–ê–ô–õ–ê ===
    if not TMDB_CSV_PATH.exists():
        logger.info("TMDB CSV –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äî —Å–∫–∞—á–∏–≤–∞–µ–º —á–µ—Ä–µ–∑ Kaggle API...")
        try:
            import kaggle
            
            kaggle_username = os.getenv("KAGGLE_USERNAME")
            kaggle_key = os.getenv("KAGGLE_KEY")
            
            if not kaggle_username or not kaggle_key:
                logger.error("KAGGLE_USERNAME –∏ KAGGLE_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")
                return None, None
            
            kaggle_dir = Path("/root/.kaggle")
            kaggle_dir.mkdir(parents=True, exist_ok=True)
            kaggle_json = kaggle_dir / "kaggle.json"
            
            if not kaggle_json.exists():
                with open(kaggle_json, "w") as f:
                    f.write(f'{{"username":"{kaggle_username}","key":"{kaggle_key}"}}')
                os.chmod(kaggle_json, 0o600)
                os.environ['KAGGLE_USERNAME'] = kaggle_username
                os.environ['KAGGLE_KEY'] = kaggle_key
            
            logger.info("–°–∫–∞—á–∏–≤–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç —á–µ—Ä–µ–∑ Kaggle API...")
            kaggle.api.dataset_download_files(
                "alanvourch/tmdb-movies-daily-updates",
                path=str(CACHE_DIR),
                unzip=True
            )
            
            actual_csv = CACHE_DIR / "TMDB_all_movies.csv"
            if not actual_csv.exists():
                logger.error("TMDB_all_movies.csv –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ—Å–ª–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è")
                logger.info(f"–°–æ–¥–µ—Ä–∂–∏–º–æ–µ CACHE_DIR: {list(CACHE_DIR.iterdir())}")
                return None, None
            
            logger.info(f"–ù–∞–π–¥–µ–Ω –≥–ª–∞–≤–Ω—ã–π —Ñ–∞–π–ª: {actual_csv.name} (—Ä–∞–∑–º–µ—Ä: {actual_csv.stat().st_size / 1e6:.1f} MB)")
            
            import shutil
            shutil.copy(actual_csv, TMDB_CSV_PATH)
            logger.info(f"TMDB CSV —É—Å–ø–µ—à–Ω–æ —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω: {TMDB_CSV_PATH}")
            
        except ImportError as e:
            logger.error(f"–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ kaggle –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞: {e}. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —á–µ—Ä–µ–∑: pip install kaggle", exc_info=True)
            return None, None
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ TMDB –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}", exc_info=True)
            return None, None

    # === –ß—Ç–µ–Ω–∏–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ CSV ===
    logger.info("–ó–∞–≥—Ä—É–∂–∞–µ–º TMDB –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ CSV...")
    try:
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã —á—Ç–µ–Ω–∏—è CSV –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Å—Ç—Ä–æ–∫
        import inspect
        sig = inspect.signature(pd.read_csv)
        
        df = None
        error = None
        
        # –ü–æ–ø—ã—Ç–∫–∞ 1: –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π pandas (>= 1.3.0) —Å on_bad_lines
        if 'on_bad_lines' in sig.parameters:
            try:
                df = pd.read_csv(
                    TMDB_CSV_PATH, 
                    low_memory=False,
                    on_bad_lines='skip',  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
                    encoding='utf-8'
                )
                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π (—Å on_bad_lines='skip')")
            except Exception as e1:
                error = e1
                logger.warning(f"–ü–æ–ø—ã—Ç–∫–∞ 1 –Ω–µ —É–¥–∞–ª–∞—Å—å: {e1}")
        
        # –ü–æ–ø—ã—Ç–∫–∞ 2: Python engine (–±–æ–ª–µ–µ –≥–∏–±–∫–∏–π –ø–∞—Ä—Å–µ—Ä)
        # –í–ê–ñ–ù–û: Python engine –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç low_memory –ø–∞—Ä–∞–º–µ—Ç—Ä
        if df is None:
            try:
                kwargs = {
                    'engine': 'python',
                    'encoding': 'utf-8'
                }
                # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Å—Ç—Ä–æ–∫
                sig = inspect.signature(pd.read_csv)
                if 'on_bad_lines' in sig.parameters:
                    kwargs['on_bad_lines'] = 'skip'
                elif 'error_bad_lines' in sig.parameters:
                    kwargs['error_bad_lines'] = False
                
                df = pd.read_csv(TMDB_CSV_PATH, **kwargs)
                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π (—á–µ—Ä–µ–∑ Python engine)")
            except Exception as e2:
                error = e2
                logger.warning(f"–ü–æ–ø—ã—Ç–∫–∞ 2 –Ω–µ —É–¥–∞–ª–∞—Å—å: {e2}")
        
        # –ü–æ–ø—ã—Ç–∫–∞ 3: –° —è–≤–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è –∫–∞–≤—ã—á–µ–∫
        # –í–ê–ñ–ù–û: Python engine –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç low_memory –ø–∞—Ä–∞–º–µ—Ç—Ä
        if df is None:
            try:
                kwargs = {
                    'engine': 'python',
                    'encoding': 'utf-8',
                    'quotechar': '"',
                    'escapechar': '\\',
                    'doublequote': True
                }
                sig = inspect.signature(pd.read_csv)
                if 'on_bad_lines' in sig.parameters:
                    kwargs['on_bad_lines'] = 'skip'
                elif 'error_bad_lines' in sig.parameters:
                    kwargs['error_bad_lines'] = False
                
                df = pd.read_csv(TMDB_CSV_PATH, **kwargs)
                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π (—Å —è–≤–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∫–∞–≤—ã—á–µ–∫)")
            except Exception as e3:
                error = e3
                logger.warning(f"–ü–æ–ø—ã—Ç–∫–∞ 3 –Ω–µ —É–¥–∞–ª–∞—Å—å: {e3}")
        
        # –ü–æ–ø—ã—Ç–∫–∞ 4: –ß—Ç–µ–Ω–∏–µ –ø–æ —á–∞—Å—Ç—è–º (chunksize) –¥–ª—è –æ–±—Ö–æ–¥–∞ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Å—Ç—Ä–æ–∫
        if df is None:
            try:
                logger.info("–ü–æ–ø—ã—Ç–∫–∞ 4: –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ –ø–æ —á–∞—Å—Ç—è–º –¥–ª—è –æ–±—Ö–æ–¥–∞ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Å—Ç—Ä–æ–∫...")
                chunks = []
                chunk_size = 10000
                skipped_rows = 0
                
                sig = inspect.signature(pd.read_csv)
                kwargs = {
                    'engine': 'python',
                    'encoding': 'utf-8',
                    'chunksize': chunk_size
                }
                if 'on_bad_lines' in sig.parameters:
                    kwargs['on_bad_lines'] = 'skip'
                elif 'error_bad_lines' in sig.parameters:
                    kwargs['error_bad_lines'] = False
                
                for chunk in pd.read_csv(TMDB_CSV_PATH, **kwargs):
                    chunks.append(chunk)
                
                if chunks:
                    df = pd.concat(chunks, ignore_index=True)
                    logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π (–ø–æ —á–∞—Å—Ç—è–º, –ø—Ä–æ–ø—É—â–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Å—Ç—Ä–æ–∫: {skipped_rows})")
                else:
                    raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞")
            except Exception as e4:
                error = e4
                logger.warning(f"–ü–æ–ø—ã—Ç–∫–∞ 4 –Ω–µ —É–¥–∞–ª–∞—Å—å: {e4}")
        
        if df is None:
            raise Exception(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å CSV –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–ø—ã—Ç–æ–∫. –ü–æ—Å–ª–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞: {error}")
        
        logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")
        logger.info(f"–ö–æ–ª–æ–Ω–∫–∏ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ: {', '.join(df.columns.tolist())}")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è CSV —Ñ–∞–π–ª–∞ –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–ø—ã—Ç–æ–∫: {e}", exc_info=True)
        return None, None
    
    # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—ã (—Ñ–æ—Ä–º–∞—Ç: 1994-06-09) –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    df['year'] = pd.to_datetime(df['release_date'], errors='coerce').dt.year
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è:
    # 1. imdb_id –Ω–µ NaN (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å)
    # 2. title –ò–õ–ò original_title –Ω–µ –ø—É—Å—Ç—ã–µ (—Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å)
    # 3. overview –ú–û–ñ–ï–¢ –±—ã—Ç—å –ø—É—Å—Ç—ã–º (–Ω–æ –±—É–¥–µ—Ç —É—á–∏—Ç—ã–≤–∞—Ç—å—Å—è –ø—Ä–∏ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏ –ø–æ–∏—Å–∫–∞)
    logger.info(f"–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è: imdb_id not NaN, (title OR original_title) not NaN")
    initial_count = len(df)
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º NaN imdb_id (–≤–∞–∂–Ω–æ: –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤ —Å—Ç—Ä–æ–∫—É)
    df = df[df['imdb_id'].notna()]
    logger.info(f"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞ imdb_id not NaN: {len(df)} —Ñ–∏–ª—å–º–æ–≤")
    
    # –¢–∞–∫–∂–µ —É–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–∫–∏ –≥–¥–µ imdb_id –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –±—É–¥–µ—Ç 'nan'
    df = df[df['imdb_id'].astype(str).str.lower() != 'nan']
    logger.info(f"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞ imdb_id != 'nan': {len(df)} —Ñ–∏–ª—å–º–æ–≤")
    
    # –§–∏–ª—å—Ç—Ä: title –ò–õ–ò original_title –Ω–µ –ø—É—Å—Ç—ã–µ
    df = df[df['title'].notna() | df['original_title'].notna()]
    logger.info(f"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞ (title OR original_title) not NaN: {len(df)} —Ñ–∏–ª—å–º–æ–≤")
    
    # –£–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ title (–Ω–æ original_title –º–æ–∂–µ—Ç –æ—Å—Ç–∞—Ç—å—Å—è)
    df = df[(df['title'].notna() & (df['title'].astype(str).str.strip() != '')) | 
            (df['original_title'].notna() & (df['original_title'].astype(str).str.strip() != ''))]
    logger.info(f"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞ (title OR original_title) not empty: {len(df)} —Ñ–∏–ª—å–º–æ–≤")
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –≥–æ–ª–æ—Å–æ–≤
    if 'vote_count' in df.columns:
        df = df[df['vote_count'] >= MIN_VOTE_COUNT]
        logger.info(f"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞ vote_count >= {MIN_VOTE_COUNT}: {len(df)} —Ñ–∏–ª—å–º–æ–≤")
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ (vote_count, –µ—Å–ª–∏ –µ—Å—Ç—å) –∏ –±–µ—Ä–µ–º —Ç–æ–ø —Ñ–∏–ª—å–º–æ–≤
    # NaN –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–¥—É—Ç –≤ –∫–æ–Ω–µ—Ü –ø—Ä–∏ ascending=False
    if 'vote_count' in df.columns:
        df = df.sort_values('vote_count', ascending=False).head(MAX_MOVIES)
    else:
        df = df.head(MAX_MOVIES)
    logger.info(f"–ü–æ—Å–ª–µ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –¥–æ {MAX_MOVIES}: {len(df)} —Ñ–∏–ª—å–º–æ–≤ (–∏–∑–Ω–∞—á–∞–ª—å–Ω–æ –±—ã–ª–æ {initial_count})")
    
    logger.info("Keywords –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Å—é–∂–µ—Ç, –∂–∞–Ω—Ä—ã, –∞–∫—Ç—ë—Ä–æ–≤, —Ä–µ–∂–∏—Å—Å—ë—Ä–∞ –∏ —Å—Ç—Ä–∞–Ω—ã –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞")
    
    df['genres_str'] = df['genres'].apply(lambda x: parse_json_list(x, 'name'))
    
    # –ê–∫—Ç—ë—Ä—ã (–ø–æ–ª–µ cast –µ—Å—Ç—å!)
    # –ü–∞—Ä—Å–∏–º cast: —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º JSON, –µ—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è - –ø–∞—Ä—Å–∏–º –∫–∞–∫ —Å—Ç—Ä–æ–∫—É —Å –∑–∞–ø—è—Ç—ã–º–∏
    def parse_cast(cast_value):
        if pd.isna(cast_value) or cast_value == '[]':
            return ''
        try:
            # –ü–æ–ø—ã—Ç–∫–∞ 1: –ø–∞—Ä—Å–∏–º –∫–∞–∫ JSON
            items = json.loads(cast_value) if isinstance(cast_value, str) else cast_value
            if isinstance(items, list):
                names = [item.get('name', '') for item in items[:10] if isinstance(item, dict) and 'name' in item]
                return ', '.join([n for n in names if n])
        except (json.JSONDecodeError, TypeError, AttributeError):
            pass
        
        # –ü–æ–ø—ã—Ç–∫–∞ 2: –ø–∞—Ä—Å–∏–º –∫–∞–∫ —Å—Ç—Ä–æ–∫—É —Å –∑–∞–ø—è—Ç—ã–º–∏
        try:
            cast_str = str(cast_value).strip()
            if cast_str and cast_str != 'nan' and not cast_str.startswith('['):
                # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∑–∞–ø—è—Ç—ã–º, –±–µ—Ä—ë–º –ø–µ—Ä–≤—ã–µ 10
                actors = [a.strip() for a in cast_str.split(',')[:10] if a.strip()]
                return ', '.join(actors)
        except (TypeError, AttributeError):
            pass
        
        return ''
    
    df['actors_str'] = df['cast'].apply(parse_cast)
    
    # –†–µ–∂–∏—Å—Å—ë—Ä—ã (–ø–æ–ª–µ director —É–∂–µ –≥–æ—Ç–æ–≤–æ –∫–∞–∫ —Å—Ç—Ä–æ–∫–∞)
    df['director_str'] = df['director'].fillna('')
    
    # –°–æ–∑–¥–∞—ë–º —Ç–æ–ø-—Å–ø–∏—Å–∫–∏ –∞–∫—Ç—ë—Ä–æ–≤ –∏ —Ä–µ–∂–∏—Å—Å—ë—Ä–æ–≤
    _create_top_lists_from_dataframe(df)
    
    # –ü—Ä–æ–¥—é—Å–µ—Ä—ã
    df['producers_str'] = df['producers'].fillna('')
    
    # –°—Ç—Ä–∞–Ω—ã –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞
    df['countries_str'] = df['production_countries'].apply(lambda x: parse_json_list(x, 'name'))
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–∞–ª–∏—á–∏–∏ overview –¥–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ
    df['has_overview'] = df['overview'].notna() & (df['overview'].astype(str).str.strip() != '')
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º title, –µ—Å–ª–∏ –µ—Å—Ç—å, –∏–Ω–∞—á–µ original_title
    df['display_title'] = df['title'].fillna(df['original_title'])
    
    df['description'] = df.apply(
        lambda row: f"{row['display_title']} ({row['year']}) {row['genres_str']}. "
                    f"{('Plot: ' + str(row['overview']) + '. ') if row.get('has_overview', False) else ''}"
                    f"Actors: {row['actors_str']}. "
                    f"Director: {row['director_str']}. "
                    f"Producers: {row['producers_str']}. "
                    f"Countries: {row['countries_str']}",
        axis=1
    )
    
    # –§–ò–ö–° IMDB ID ‚Äî —á–∏—Å—Ç–∏–º .0 –∏ —É–±–∏—Ä–∞–µ–º –≤—Å–µ tt –≤ –Ω–∞—á–∞–ª–µ (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –ë–ï–ó –ø—Ä–µ—Ñ–∏–∫—Å–∞ tt)
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–æ–ª—å–∫–æ –∫ –≤–∞–ª–∏–¥–Ω—ã–º imdb_id (–Ω–µ NaN, –Ω–µ –ø—É—Å—Ç—ã–µ)
    df['imdb_id'] = df['imdb_id'].astype(str).str.strip()  # —É–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã
    df['imdb_id'] = df['imdb_id'].str.replace(r'\.0$', '', regex=True)  # —É–±–∏—Ä–∞–µ–º .0
    # –£–±–∏—Ä–∞–µ–º –≤—Å–µ "tt" –≤ –Ω–∞—á–∞–ª–µ (–º–æ–∂–µ—Ç –±—ã—Ç—å tttt –∏–ª–∏ tt), —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ë–ï–ó –ø—Ä–µ—Ñ–∏–∫—Å–∞
    df['imdb_id'] = df['imdb_id'].str.replace(r'^tt+', '', regex=True)  # —É–±–∏—Ä–∞–µ–º –≤—Å–µ tt –≤ –Ω–∞—á–∞–ª–µ
    
    # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏, –≥–¥–µ imdb_id —Å—Ç–∞–ª –ø—É—Å—Ç—ã–º –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    df = df[df['imdb_id'].str.len() > 0]
    logger.info(f"–ü–æ—Å–ª–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –æ—á–∏—Å—Ç–∫–∏ imdb_id: {len(df)} —Ñ–∏–ª—å–º–æ–≤")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º has_overview, actors_str, director_str, genres_str, overview –∏ vote_count –¥–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏ –∏ keyword-–º–∞—Ç—á–∏–Ω–≥–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ
    # overview —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è keyword-–º–∞—Ç—á–∏–Ω–≥–∞ (—Å–∞–º—ã–π —Å–∏–ª—å–Ω—ã–π –±—É—Å—Ç)
    # vote_count –Ω—É–∂–µ–Ω –¥–ª—è –±—É—Å—Ç–∞ –ø–æ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏
    columns_to_save = ['imdb_id', 'title', 'year', 'description', 'has_overview', 'actors_str', 'director_str', 'genres_str', 'overview']
    if 'vote_count' in df.columns:
        columns_to_save.append('vote_count')
    processed = df[columns_to_save].copy()
    # –ó–∞–º–µ–Ω—è–µ–º NaN –Ω–∞ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–π —Ä–∞–±–æ—Ç—ã
    processed['overview'] = processed['overview'].fillna('')
    processed['genres_str'] = processed['genres_str'].fillna('')
    # –£–∂–µ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–ª–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–ª–∏ –≤—ã—à–µ, –Ω–µ –Ω—É–∂–Ω–æ –µ—â–µ —Ä–∞–∑ .head()
    
    # –ö–≠–®–ò–†–û–í–ê–ù–ò–ï: –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –±—ã–ª–∏ –ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —É–∂–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã
    # –ï—Å–ª–∏ –∏–Ω–¥–µ–∫—Å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –∑–Ω–∞—á–∏—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —É–∂–µ –≤—ã—á–∏—Å–ª–µ–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã
    if INDEX_PATH.exists() and DATA_PATH.exists():
        logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç ({INDEX_PATH}) - —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —É–∂–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
        logger.info("–ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω–¥–µ–∫—Å –∏–∑ —Ñ–∞–π–ª–∞ –≤–º–µ—Å—Ç–æ –ø–µ—Ä–µ–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        try:
            _index = faiss.read_index(str(INDEX_PATH))
            _movies_df = pd.read_csv(DATA_PATH)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —Å —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª—å—é
            model = get_model()
            expected_dim = model.get_sentence_embedding_dimension()
            actual_dim = _index.d
            
            if expected_dim != actual_dim:
                logger.warning(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∏–Ω–¥–µ–∫—Å–∞ ({actual_dim}) –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –º–æ–¥–µ–ª—å—é ({expected_dim}) - –ø–µ—Ä–µ—Å–æ–±–∏—Ä–∞–µ–º")
                _index = None
                _movies_df = None
            else:
                logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ –∫—ç—à–∞, —Ñ–∏–ª—å–º–æ–≤: {len(_movies_df)}, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {actual_dim}")
                return _index, _movies_df
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞: {e}, –ø–µ—Ä–µ—Å–æ–±–∏—Ä–∞–µ–º...", exc_info=True)
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –∏–Ω–¥–µ–∫—Å –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª—Å—è
    logger.info(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(processed)} —Ñ–∏–ª—å–º–æ–≤...")
    logger.info("‚ö†Ô∏è –≠—Ç–æ –∑–∞–π–º–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç. –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –∫—ç—à –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–∞–ø—É—Å–∫–∞.")
    
    model = get_model()
    descriptions = processed['description'].tolist()
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º batch_size –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è (–º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è)
    # –î–ª—è Railway —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 64-128 (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏)
    # –î–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π –º–∞—à–∏–Ω—ã —Å GPU –º–æ–∂–Ω–æ 256-512
    batch_size = int(os.getenv('EMBEDDINGS_BATCH_SIZE', '64'))
    logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è batch_size={batch_size} –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
    logger.info(f"üí° –°–æ–≤–µ—Ç: –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —É—Å–∫–æ—Ä–µ–Ω–∏—è –Ω–∞ Railway —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ USE_FAST_EMBEDDINGS=1 –∏ EMBEDDINGS_BATCH_SIZE=128")
    
    embeddings = []
    total_batches = (len(descriptions) + batch_size - 1) // batch_size
    logger.info(f"–í—Å–µ–≥–æ –±–∞—Ç—á–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {total_batches}")
    
    for i in tqdm(range(0, len(descriptions), batch_size), desc="Embeddings", total=total_batches):
        batch = descriptions[i:i+batch_size]
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è: convert_to_numpy=True, normalize_embeddings=False
        batch_emb = model.encode(
            batch, 
            show_progress_bar=False,
            convert_to_numpy=True,
            normalize_embeddings=False,
            batch_size=batch_size
        )
        embeddings.extend(batch_emb)
        
        # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –ª–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        if (i // batch_size + 1) % 10 == 0:
            logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i + len(batch)}/{len(descriptions)} —Ñ–∏–ª—å–º–æ–≤ ({(i + len(batch)) / len(descriptions) * 100:.1f}%)")
    
    embeddings = np.array(embeddings).astype('float32')
    dimension = embeddings.shape[1]
    
    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å –≤ –∫—ç—à –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–∞–ø—É—Å–∫–∞
    logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –≤ –∫—ç—à: {INDEX_PATH}")
    faiss.write_index(index, str(INDEX_PATH))
    processed.to_csv(DATA_PATH, index=False)
    logger.info("‚úÖ –ò–Ω–¥–µ–∫—Å –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –∫—ç—à")
    
    _index = index
    _movies_df = processed
    
    logger.info(f"–ì–æ—Ç–æ–≤–æ! –°–æ–∑–¥–∞–Ω –∏–Ω–¥–µ–∫—Å –Ω–∞ {len(processed)} —Ñ–∏–ª—å–º–æ–≤")
    return index, processed

def load_top_actors_and_directors():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–æ–ø-500 –∞–∫—Ç—ë—Ä–æ–≤ –∏ —Ç–æ–ø-100 —Ä–µ–∂–∏—Å—Å—ë—Ä–æ–≤ –∏–∑ —Ñ–∞–π–ª–æ–≤"""
    global _top_actors_set, _top_directors_set
    
    if _top_actors_set is not None and _top_directors_set is not None:
        return _top_actors_set, _top_directors_set
    
    _top_actors_set = set()
    _top_directors_set = set()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ø-500 –∞–∫—Ç—ë—Ä–æ–≤
    logger.info(f"[LOAD TOP LISTS] –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–∞ —Ç–æ–ø-–∞–∫—Ç—ë—Ä–æ–≤: {TOP_ACTORS_PATH}")
    logger.info(f"[LOAD TOP LISTS] –§–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç? {TOP_ACTORS_PATH.exists()}")
    if TOP_ACTORS_PATH.exists():
        try:
            file_size = TOP_ACTORS_PATH.stat().st_size
            logger.info(f"[LOAD TOP LISTS] –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ —Ç–æ–ø-–∞–∫—Ç—ë—Ä–æ–≤: {file_size} –±–∞–π—Ç")
            with open(TOP_ACTORS_PATH, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                logger.info(f"[LOAD TOP LISTS] –ü—Ä–æ—á–∏—Ç–∞–Ω–æ —Å—Ç—Ä–æ–∫ –∏–∑ —Ñ–∞–π–ª–∞: {len(lines)}")
                _top_actors_set = {line.strip().lower() for line in lines if line.strip()}
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(_top_actors_set)} –∞–∫—Ç—ë—Ä–æ–≤ –∏–∑ —Ç–æ–ø-500")
            if len(_top_actors_set) > 0:
                logger.info(f"   –ü—Ä–∏–º–µ—Ä—ã –ø–µ—Ä–≤—ã—Ö 5 –∞–∫—Ç—ë—Ä–æ–≤: {list(_top_actors_set)[:5]}")
            else:
                logger.error(f"‚ùå –§–∞–π–ª —Ç–æ–ø-–∞–∫—Ç—ë—Ä–æ–≤ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –Ω–æ –ø—É—Å—Ç–æ–π! –†–∞–∑–º–µ—Ä: {file_size} –±–∞–π—Ç")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–ø-–∞–∫—Ç—ë—Ä–æ–≤: {e}", exc_info=True)
            _top_actors_set = set()
    else:
        logger.error(f"‚ùå –§–∞–π–ª —Ç–æ–ø-–∞–∫—Ç—ë—Ä–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω: {TOP_ACTORS_PATH}")
        logger.error(f"   –ê–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å: {TOP_ACTORS_PATH.absolute()}")
        logger.error(f"   –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç? {TOP_ACTORS_PATH.parent.exists()}")
        if TOP_ACTORS_PATH.parent.exists():
            logger.error(f"   –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {list(TOP_ACTORS_PATH.parent.iterdir())}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ø-100 —Ä–µ–∂–∏—Å—Å—ë—Ä–æ–≤
    logger.info(f"[LOAD TOP LISTS] –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–∞ —Ç–æ–ø-—Ä–µ–∂–∏—Å—Å—ë—Ä–æ–≤: {TOP_DIRECTORS_PATH}")
    logger.info(f"[LOAD TOP LISTS] –§–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç? {TOP_DIRECTORS_PATH.exists()}")
    if TOP_DIRECTORS_PATH.exists():
        try:
            file_size = TOP_DIRECTORS_PATH.stat().st_size
            logger.info(f"[LOAD TOP LISTS] –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ —Ç–æ–ø-—Ä–µ–∂–∏—Å—Å—ë—Ä–æ–≤: {file_size} –±–∞–π—Ç")
            with open(TOP_DIRECTORS_PATH, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                logger.info(f"[LOAD TOP LISTS] –ü—Ä–æ—á–∏—Ç–∞–Ω–æ —Å—Ç—Ä–æ–∫ –∏–∑ —Ñ–∞–π–ª–∞: {len(lines)}")
                _top_directors_set = {line.strip().lower() for line in lines if line.strip()}
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(_top_directors_set)} —Ä–µ–∂–∏—Å—Å—ë—Ä–æ–≤ –∏–∑ —Ç–æ–ø-100")
            if len(_top_directors_set) > 0:
                logger.info(f"   –ü—Ä–∏–º–µ—Ä—ã –ø–µ—Ä–≤—ã—Ö 5 —Ä–µ–∂–∏—Å—Å—ë—Ä–æ–≤: {list(_top_directors_set)[:5]}")
            else:
                logger.error(f"‚ùå –§–∞–π–ª —Ç–æ–ø-—Ä–µ–∂–∏—Å—Å—ë—Ä–æ–≤ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –Ω–æ –ø—É—Å—Ç–æ–π! –†–∞–∑–º–µ—Ä: {file_size} –±–∞–π—Ç")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–ø-—Ä–µ–∂–∏—Å—Å—ë—Ä–æ–≤: {e}", exc_info=True)
            _top_directors_set = set()
    else:
        logger.error(f"‚ùå –§–∞–π–ª —Ç–æ–ø-—Ä–µ–∂–∏—Å—Å—ë—Ä–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω: {TOP_DIRECTORS_PATH}")
        logger.error(f"   –ê–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å: {TOP_DIRECTORS_PATH.absolute()}")
    
    return _top_actors_set, _top_directors_set


def get_index_and_movies():
    global _index, _movies_df
    
    logger.info("[GET INDEX] –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–π –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∏
    force_rebuild = os.getenv('FORCE_REBUILD_INDEX', '0').strip().lower() in ('1', 'true', 'yes', 'on')
    if force_rebuild:
        logger.warning("[GET INDEX] ‚ö†Ô∏è FORCE_REBUILD_INDEX=1 - –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∞ –∏–Ω–¥–µ–∫—Å–∞!")
        # –£–¥–∞–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å –∏ –¥–∞–Ω–Ω—ã–µ
        try:
            if INDEX_PATH.exists():
                INDEX_PATH.unlink()
                logger.info("[GET INDEX] –£–¥–∞–ª–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å")
            if DATA_PATH.exists():
                DATA_PATH.unlink()
                logger.info("[GET INDEX] –£–¥–∞–ª–µ–Ω—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ")
        except Exception as e:
            logger.warning(f"[GET INDEX] –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ —Å—Ç–∞—Ä–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞: {e}")
        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        _index = None
        _movies_df = None
    
    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –±–µ–∑ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏, –µ—Å–ª–∏ –∏–Ω–¥–µ–∫—Å —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω
    if _index is not None and _movies_df is not None:
        logger.info(f"[GET INDEX] –ò–Ω–¥–µ–∫—Å —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω –≤ –ø–∞–º—è—Ç–∏, —Ñ–∏–ª—å–º–æ–≤: {len(_movies_df)}")
        return _index, _movies_df
    
    logger.info("[GET INDEX] –ò–Ω–¥–µ–∫—Å –Ω–µ –≤ –ø–∞–º—è—Ç–∏, –ø—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å...")
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –î–û –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ –∏–Ω–¥–µ–∫—Å–∞, —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å –¥—Ä—É–≥–∏–µ –ø–æ—Ç–æ–∫–∏
    # –≠—Ç–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ, —Ç–∞–∫ –∫–∞–∫ get_model() –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–≤–æ—é –±–ª–æ–∫–∏—Ä–æ–≤–∫—É
    if _model is None:
        logger.info("[GET INDEX] –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –ø–µ—Ä–µ–¥ –±–ª–æ–∫–∏—Ä–æ–≤–∫–æ–π –∏–Ω–¥–µ–∫—Å–∞...")
        get_model()
    
    with _index_lock:  # ‚Üê –¢–æ–ª—å–∫–æ –æ–¥–∏–Ω worker –º–æ–∂–µ—Ç –≤–æ–π—Ç–∏ —Å—é–¥–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
        logger.info("[GET INDEX] –ü–æ–ª—É—á–µ–Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–µ–∫—Å–∞...")
        # –î–≤–æ–π–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - –≤–æ–∑–º–æ–∂–Ω–æ, –¥—Ä—É–≥–æ–π –ø–æ—Ç–æ–∫ —É–∂–µ –∑–∞–≥—Ä—É–∑–∏–ª –∏–Ω–¥–µ–∫—Å
        if _index is not None and _movies_df is not None:
            logger.info(f"[GET INDEX] –ò–Ω–¥–µ–∫—Å —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω –¥—Ä—É–≥–∏–º –ø–æ—Ç–æ–∫–æ–º, —Ñ–∏–ª—å–º–æ–≤: {len(_movies_df)}")
            return _index, _movies_df
        
        logger.info("[GET INDEX] –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω–¥–µ–∫—Å —á–µ—Ä–µ–∑ build_tmdb_index()...")
        try:
            _index, _movies_df = build_tmdb_index()
            if _index is not None and _movies_df is not None:
                logger.info(f"[GET INDEX] –ò–Ω–¥–µ–∫—Å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω, —Ñ–∏–ª—å–º–æ–≤: {len(_movies_df)}")
                # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ø-—Å–ø–∏—Å–∫–∏ –∞–∫—Ç—ë—Ä–æ–≤ –∏ —Ä–µ–∂–∏—Å—Å—ë—Ä–æ–≤
                load_top_actors_and_directors()
            else:
                logger.warning("[GET INDEX] build_tmdb_index() –≤–µ—Ä–Ω—É–ª None")
            return _index, _movies_df
        except Exception as e:
            logger.error(f"[GET INDEX] –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏–Ω–¥–µ–∫—Å–∞: {e}", exc_info=True)
            return None, None

def _normalize_text(text):
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–µ–∫—Å—Ç: –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∏ —É–±–∏—Ä–∞–µ—Ç –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è"""
    import re
    # –£–±–∏—Ä–∞–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è, –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã –∏ —Ü–∏—Ñ—Ä—ã, –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    normalized = re.sub(r'[^\w\s]', '', str(text).lower())
    return normalized


def _get_actor_position(actors_str, actor_name_normalized):
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø–æ–∑–∏—Ü–∏—é –∞–∫—Ç—ë—Ä–∞ –≤ —Å–ø–∏—Å–∫–µ –∞–∫—Ç—ë—Ä–æ–≤ —Ñ–∏–ª—å–º–∞.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–∑–∏—Ü–∏—é (1-based) –∏–ª–∏ None, –µ—Å–ª–∏ –∞–∫—Ç—ë—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω.
    """
    if not actors_str or pd.isna(actors_str):
        return None
    
    # –ü–∞—Ä—Å–∏–º —Å–ø–∏—Å–æ–∫ –∞–∫—Ç—ë—Ä–æ–≤ (—Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å - –∑–∞–ø—è—Ç–∞—è)
    actors_list = [a.strip().lower() for a in str(actors_str).split(',')]
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫–∞–∂–¥–æ–≥–æ –∞–∫—Ç—ë—Ä–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    for idx, actor in enumerate(actors_list):
        actor_normalized = _normalize_text(actor)
        if actor_normalized == actor_name_normalized:
            return idx + 1  # 1-based –ø–æ–∑–∏—Ü–∏—è
    
    return None


def _get_genre_keywords():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±–ª–∞–∫–∞ —Å–º—ã—Å–ª–æ–≤ –¥–ª—è –∂–∞–Ω—Ä–æ–≤ (—Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ —Å–ª–æ–≤–∞ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º)"""
    return {
        'action': [
            'shootout', 'chase', 'fight', 'danger', 'killer', 'villain', 'explosion', 'gun', 'weapon', 'battle',
            'combat', 'war', 'soldier', 'spy', 'agent', 'mission', 'rescue', 'escape', 'pursuit', 'conflict',
            'violence', 'action', 'thriller', 'adrenaline', 'stunt', 'hero', 'enemy', 'attack', 'defense', 'survival'
        ],
        'comedy': [
            'funny', 'laugh', 'humor', 'joke', 'comic', 'hilarious', 'amusing', 'entertaining', 'light', 'cheerful',
            'silly', 'witty', 'satire', 'parody', 'romantic comedy', 'slapstick', 'absurd', 'quirky', 'playful',
            'humorous', 'comedy', 'fun', 'gag', 'prank', 'mischief', 'comical', 'laughable', 'ridiculous', 'wacky'
        ],
        'thriller': [
            'suspense', 'tension', 'mystery', 'intrigue', 'plot', 'twist', 'surprise', 'suspicious', 'dangerous',
            'threatening', 'fear', 'anxiety', 'nervous', 'edge', 'cliffhanger', 'unpredictable', 'shocking',
            'disturbing', 'psychological', 'thriller', 'suspenseful', 'nail-biting', 'gripping', 'intense',
            'chilling', 'terrifying', 'ominous', 'sinister', 'menacing', 'alarming'
        ],
        'drama': [
            'emotional', 'serious', 'tragic', 'melodrama', 'conflict', 'struggle', 'relationship', 'family',
            'love', 'loss', 'grief', 'sorrow', 'pain', 'suffering', 'human', 'realistic', 'deep', 'meaningful',
            'touching', 'heartfelt', 'dramatic', 'intense', 'powerful', 'moving', 'profound', 'thoughtful',
            'contemplative', 'reflective', 'poignant', 'heartbreaking'
        ],
        'horror': [
            'scary', 'frightening', 'terrifying', 'horror', 'monster', 'ghost', 'demon', 'zombie', 'vampire',
            'killer', 'murder', 'death', 'blood', 'gore', 'nightmare', 'fear', 'terror', 'panic', 'dread',
            'creepy', 'spooky', 'eerie', 'sinister', 'dark', 'evil', 'supernatural', 'paranormal', 'haunted',
            'disturbing', 'shocking', 'gruesome'
        ],
        'romance': [
            'love', 'romance', 'romantic', 'relationship', 'couple', 'dating', 'wedding', 'marriage', 'kiss',
            'passion', 'affection', 'heart', 'soulmate', 'sweet', 'tender', 'intimate', 'emotional', 'caring',
            'devoted', 'loving', 'adoring', 'charming', 'enchanting', 'beautiful', 'dreamy', 'sentimental',
            'touching', 'heartwarming', 'endearing', 'affectionate'
        ],
        'animation': [
            'cartoon', 'animated', 'animation', 'drawing', 'illustration', 'picture', 'graphic', 'visual',
            'artistic', 'creative', 'colorful', 'vibrant', 'fantasy', 'imaginative', 'whimsical', 'playful',
            'childlike', 'innocent', 'magical', 'enchanting', 'fairy tale', 'storybook', 'pixar', 'disney',
            'family', 'children', 'kids', 'youthful', 'cheerful', 'bright', 'lively'
        ],
        'crime': [
            'crime', 'criminal', 'gangster', 'mafia', 'police', 'detective', 'investigation', 'murder', 'killing',
            'robbery', 'theft', 'corruption', 'illegal', 'law', 'justice', 'prison', 'criminal', 'felony',
            'violence', 'danger', 'suspense', 'mystery', 'thriller', 'underworld', 'organized crime', 'heist',
            'conspiracy', 'betrayal', 'revenge', 'punishment'
        ],
        'sci-fi': [
            'science fiction', 'sci-fi', 'future', 'space', 'alien', 'robot', 'technology', 'advanced', 'scientific',
            'futuristic', 'spacecraft', 'planet', 'galaxy', 'universe', 'time travel', 'dystopia', 'utopia',
            'cyberpunk', 'artificial intelligence', 'genetic', 'experiment', 'discovery', 'innovation', 'virtual',
            'digital', 'quantum', 'dimension', 'parallel', 'extraterrestrial', 'cosmic'
        ],
        'adventure': [
            'adventure', 'journey', 'quest', 'expedition', 'exploration', 'discovery', 'treasure', 'hunt',
            'travel', 'voyage', 'expedition', 'explorer', 'hero', 'brave', 'courageous', 'daring', 'bold',
            'exciting', 'thrilling', 'action', 'danger', 'risk', 'challenge', 'mission', 'goal', 'destination',
            'unknown', 'mysterious', 'exotic', 'foreign'
        ]
    }


def _detect_genre_from_keywords(keywords, query_en_lower):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∂–∞–Ω—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏ –æ–±–ª–∞–∫–æ–≤ —Å–º—ã—Å–ª–æ–≤"""
    genre_keywords_map = _get_genre_keywords()
    detected_genres = []
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥–æ–µ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –Ω–∞ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç—å –∫ –∂–∞–Ω—Ä–∞–º
    for genre, genre_words in genre_keywords_map.items():
        matches = sum(1 for word in keywords if word in genre_words)
        # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤–µ—Å—å –∑–∞–ø—Ä–æ—Å –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã—Ö —Å–ª–æ–≤
        query_matches = sum(1 for word in genre_words if word in query_en_lower)
        total_matches = matches + query_matches
        
        if total_matches >= 2:  # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–æ 2+ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è - –∂–∞–Ω—Ä –æ–±–Ω–∞—Ä—É–∂–µ–Ω
            detected_genres.append(genre)
            logger.info(f"[SEARCH MOVIES] –û–±–Ω–∞—Ä—É–∂–µ–Ω –∂–∞–Ω—Ä '{genre}' –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (—Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {total_matches})")
    
    return detected_genres

def _extract_keywords(query_en):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞, —É–±–∏—Ä–∞—è —Å—Ç–æ–ø-—Å–ª–æ–≤–∞"""
    # –°—Ç–æ–ø-—Å–ª–æ–≤–∞ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø—Ä–∏ keyword-–º–∞—Ç—á–∏–Ω–≥–µ)
    stop_words = {
        'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
        'from', 'up', 'about', 'into', 'through', 'during', 'including', 'against', 'among',
        'throughout', 'despite', 'towards', 'upon', 'concerning', 'to', 'of', 'in', 'for',
        'film', 'movie', 'films', 'movies', 'plays', 'playing', 'actor', 'actors', 'director',
        'directors', 'starring', 'star', 'stars', 'cast', 'about', 'with', 'in', 'a', 'the'
    }
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∑–∞–ø—Ä–æ—Å (—É–±–∏—Ä–∞–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é, –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É)
    normalized_query = _normalize_text(query_en)
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
    import re
    words = re.findall(r'\b\w+\b', normalized_query)
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ (–º–µ–Ω—å—à–µ 2 —Å–∏–º–≤–æ–ª–æ–≤)
    keywords = [w for w in words if w not in stop_words and len(w) > 2]
    logger.info(f"[SEARCH MOVIES] –ò–∑–≤–ª–µ—á–µ–Ω—ã –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ '{query_en}': {keywords}")
    return keywords


def search_movies(query, top_k=15):
    try:
        logger.info(f"[SEARCH MOVIES] –ù–∞—á–∞–ª–æ –ø–æ–∏—Å–∫–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{query}' (FUZZINESS_LEVEL={FUZZINESS_LEVEL})")
        
        logger.info(f"[SEARCH MOVIES] –®–∞–≥ 1: –ü–µ—Ä–µ–≤–æ–¥ –∑–∞–ø—Ä–æ—Å–∞...")
        query_en = translate_to_english(query)
        logger.info(f"[SEARCH MOVIES] –ü–µ—Ä–µ–≤–µ–¥–µ–Ω–æ: '{query}' ‚Üí '{query_en}'")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –æ–±—ã—á–Ω–æ–≥–æ keyword-–º–∞—Ç—á–∏–Ω–≥–∞
        keywords = _extract_keywords(query_en)
        query_en_lower = query_en.lower()
        query_en_words = query_en_lower.split()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ø-—Å–ø–∏—Å–∫–∏ –∞–∫—Ç—ë—Ä–æ–≤ –∏ —Ä–µ–∂–∏—Å—Å—ë—Ä–æ–≤
        top_actors_set, top_directors_set = load_top_actors_and_directors()
        
        # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –∏–º—è –∞–∫—Ç—ë—Ä–∞/—Ä–µ–∂–∏—Å—Å—ë—Ä–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Å–ª–æ–≤ (2-4 —Å–ª–æ–≤–∞)
        mentioned_actor_en = None
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –≤ —Ç–æ–ø-—Å–ø–∏—Å–∫–∞—Ö (–µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ –ø—É—Å—Ç—ã–µ)
        if top_actors_set or top_directors_set:
            for word_count in range(2, min(5, len(query_en_words) + 1)):
                for i in range(len(query_en_words) - word_count + 1):
                    potential_name = ' '.join(query_en_words[i:i+word_count])
                    potential_name_normalized = _normalize_text(potential_name)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤ —Ç–æ–ø-–∞–∫—Ç—ë—Ä–∞—Ö (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç ‚Ññ1)
                    if top_actors_set and potential_name_normalized in top_actors_set:
                        mentioned_actor_en = potential_name_normalized
                        logger.info(f"[SEARCH MOVIES] –ù–∞–π–¥–µ–Ω–æ –∏–º—è –∞–∫—Ç—ë—Ä–∞ –≤ —Ç–æ–ø-500: '{mentioned_actor_en}' ({word_count} —Å–ª–æ–≤–∞)")
                        break
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤ —Ç–æ–ø-—Ä–µ–∂–∏—Å—Å—ë—Ä–∞—Ö (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç ‚Ññ2, —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –∞–∫—Ç—ë—Ä–∞ –Ω–µ –Ω–∞—à–ª–∏)
                    if not mentioned_actor_en and top_directors_set and potential_name_normalized in top_directors_set:
                        mentioned_actor_en = potential_name_normalized
                        logger.info(f"[SEARCH MOVIES] –ù–∞–π–¥–µ–Ω–æ –∏–º—è —Ä–µ–∂–∏—Å—Å—ë—Ä–∞ –≤ —Ç–æ–ø-100: '{mentioned_actor_en}' ({word_count} —Å–ª–æ–≤–∞)")
                        break
                
                if mentioned_actor_en:
                    break
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ —Ç–æ–ø-—Å–ø–∏—Å–∫–∞—Ö, –ù–ï –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º —á—Ç–æ —ç—Ç–æ –∞–∫—Ç—ë—Ä
        # –¢–æ–ø-—Å–ø–∏—Å–æ–∫ –∞–∫—Ç—ë—Ä–æ–≤ - —ç—Ç–æ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –∏—Å—Ç–∏–Ω—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∞–∫—Ç—ë—Ä–æ–≤
        if not mentioned_actor_en:
            logger.info(f"[SEARCH MOVIES] –ò–º—è –∞–∫—Ç—ë—Ä–∞/—Ä–µ–∂–∏—Å—Å—ë—Ä–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ —Ç–æ–ø-—Å–ø–∏—Å–∫–∞—Ö - –Ω–µ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º —á—Ç–æ —ç—Ç–æ –∞–∫—Ç—ë—Ä")
        
        logger.info(f"[SEARCH MOVIES] –£–ø–æ–º—è–Ω—É—Ç –∞–∫—Ç—ë—Ä/—Ä–µ–∂–∏—Å—Å—ë—Ä? {bool(mentioned_actor_en)}, –∏–º—è (en): {mentioned_actor_en}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∂–∞–Ω—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏ –æ–±–ª–∞–∫–æ–≤ —Å–º—ã—Å–ª–æ–≤
        detected_genres = _detect_genre_from_keywords(keywords, query_en_lower)
        logger.info(f"[SEARCH MOVIES] –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∂–∞–Ω—Ä—ã –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º: {detected_genres}")
        
        logger.info(f"[SEARCH MOVIES] –®–∞–≥ 2: –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –∏ –¥–∞–Ω–Ω—ã—Ö...")
        index, movies = get_index_and_movies()
        if index is None:
            logger.warning("[SEARCH MOVIES] –ò–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫")
            return []
        logger.info(f"[SEARCH MOVIES] –ò–Ω–¥–µ–∫—Å –ø–æ–ª—É—á–µ–Ω, —Ñ–∏–ª—å–º–æ–≤: {len(movies)}")
        
        logger.info(f"[SEARCH MOVIES] –®–∞–≥ 3: –ü–æ–ª—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ embeddings...")
        model = get_model()
        logger.info(f"[SEARCH MOVIES] –ú–æ–¥–µ–ª—å –ø–æ–ª—É—á–µ–Ω–∞")
        
        logger.info(f"[SEARCH MOVIES] –®–∞–≥ 4: –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–∞...")
        query_emb = model.encode([query_en])[0].astype('float32').reshape(1, -1)
        logger.info(f"[SEARCH MOVIES] –≠–º–±–µ–¥–¥–∏–Ω–≥ —Å–æ–∑–¥–∞–Ω, —Ä–∞–∑–º–µ—Ä: {query_emb.shape}")
        
        logger.info(f"[SEARCH MOVIES] –®–∞–≥ 5: –ü–æ–∏—Å–∫ –≤ –∏–Ω–¥–µ–∫—Å–µ...")
        
        query_dim = query_emb.shape[1]
        index_dim = index.d
        if query_dim != index_dim:
            logger.error(f"[SEARCH MOVIES] –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–∞ ({query_dim}) –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∏–Ω–¥–µ–∫—Å–æ–º ({index_dim})!")
            return []
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ FUZZINESS_LEVEL
        # FUZZINESS_LEVEL –≤–ª–∏—è–µ—Ç –Ω–∞ —à–∏—Ä–∏–Ω—É –ø–æ–∏—Å–∫–∞:
        # 0 = top_k * 3 (—Å—Ç—Ä–æ–≥–∏–π –ø–æ–∏—Å–∫)
        # 50 = top_k * 5 (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
        # 100 = top_k * 10 (–º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫)
        fuzziness_multiplier = 3 + (FUZZINESS_LEVEL / 100.0) * 7  # –û—Ç 3 –¥–æ 10
        search_k = int(top_k * fuzziness_multiplier)
        logger.info(f"[SEARCH MOVIES] FUZZINESS_LEVEL={FUZZINESS_LEVEL}, multiplier={fuzziness_multiplier:.2f}, search_k={search_k}")
        
        # –ï—Å–ª–∏ –∞–∫—Ç—ë—Ä/—Ä–µ–∂–∏—Å—Å—ë—Ä —É–ø–æ–º—è–Ω—É—Ç ‚Äî —Å–Ω–∞—á–∞–ª–∞ –Ω–∞—Ö–æ–¥–∏–º –í–°–ï –µ–≥–æ —Ñ–∏–ª—å–º—ã –≤–æ –≤—Å–µ–π –±–∞–∑–µ
        # –ó–∞—Ç–µ–º —Ä–∞–Ω–∂–∏—Ä—É–µ–º –∏—Ö –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –∑–∞–ø—Ä–æ—Å–∞
        candidate_indices = []
        candidate_distances = []
        actor_movie_indices = []  # –ò–Ω–¥–µ–∫—Å—ã –≤—Å–µ—Ö —Ñ–∏–ª—å–º–æ–≤ —Å –∞–∫—Ç—ë—Ä–æ–º (–¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è)
        
        if mentioned_actor_en:
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏–º—è –¥–ª—è –ø–æ–∏—Å–∫–∞ (—É–±–∏—Ä–∞–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é, –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É)
            actor_name_for_search = _normalize_text(mentioned_actor_en)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∏–º—è –∞–∫—Ç—ë—Ä–æ–º –∏–ª–∏ —Ä–µ–∂–∏—Å—Å—ë—Ä–æ–º –∏–∑ —Ç–æ–ø-—Å–ø–∏—Å–∫–æ–≤
            is_actor = top_actors_set and actor_name_for_search in top_actors_set
            is_director = top_directors_set and actor_name_for_search in top_directors_set
            
            if not is_actor and not is_director:
                logger.warning(f"[SEARCH MOVIES] –ò–º—è '{actor_name_for_search}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ —Ç–æ–ø-—Å–ø–∏—Å–∫–∞—Ö, –Ω–æ mentioned_actor_en –Ω–µ None - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –ø–æ –∞–∫—Ç—ë—Ä—É")
                mentioned_actor_en = None
            else:
                # –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: –ò—â–µ–º –í–°–ï —Ñ–∏–ª—å–º—ã —Å —ç—Ç–∏–º –∞–∫—Ç—ë—Ä–æ–º/—Ä–µ–∂–∏—Å—Å—ë—Ä–æ–º –≤–æ –≤—Å–µ–π –±–∞–∑–µ
                logger.info(f"[SEARCH MOVIES] –ü–æ–∏—Å–∫ –í–°–ï–• —Ñ–∏–ª—å–º–æ–≤ —Å {'–∞–∫—Ç—ë—Ä–æ–º' if is_actor else '—Ä–µ–∂–∏—Å—Å—ë—Ä–æ–º'} '{actor_name_for_search}' –≤–æ –≤—Å–µ–π –±–∞–∑–µ...")
                
                for idx in range(len(movies)):
                    row = movies.iloc[idx]
                    
                    # –ü–†–ò–û–†–ò–¢–ï–¢ ‚Ññ1: –ê–∫—Ç—ë—Ä—ã
                    if is_actor and 'actors_str' in row.index:
                        actors_str = row.get('actors_str', '')
                        if pd.notna(actors_str) and actor_name_for_search in _normalize_text(actors_str):
                            actor_movie_indices.append(idx)
                    
                    # –ü–†–ò–û–†–ò–¢–ï–¢ ‚Ññ2: –†–µ–∂–∏—Å—Å—ë—Ä—ã (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ –∞–∫—Ç—ë—Ä)
                    elif is_director and not is_actor and 'director_str' in row.index:
                        director_str = row.get('director_str', '')
                        if pd.notna(director_str) and actor_name_for_search in _normalize_text(director_str):
                            actor_movie_indices.append(idx)
                
                logger.info(f"[SEARCH MOVIES] –ù–∞–π–¥–µ–Ω–æ –í–°–ï–ì–û —Ñ–∏–ª—å–º–æ–≤ —Å {'–∞–∫—Ç—ë—Ä–æ–º' if is_actor else '—Ä–µ–∂–∏—Å—Å—ë—Ä–æ–º'} '{actor_name_for_search}': {len(actor_movie_indices)}")
                
                if actor_movie_indices:
                    # –¢–µ–ø–µ—Ä—å —Ä–∞–Ω–∂–∏—Ä—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ–∏–ª—å–º—ã –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É —Å –∑–∞–ø—Ä–æ—Å–æ–º
                    # –°–æ–∑–¥–∞—ë–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ñ–∏–ª—å–º–æ–≤
                    actor_movie_descriptions = []
                    valid_indices = []
                    
                    for idx in actor_movie_indices:
                        row = movies.iloc[idx]
                        description = row.get('description', '')
                        if pd.notna(description) and description:
                            actor_movie_descriptions.append(description)
                            valid_indices.append(idx)
                    
                    if actor_movie_descriptions:
                        logger.info(f"[SEARCH MOVIES] –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è {len(actor_movie_descriptions)} —Ñ–∏–ª—å–º–æ–≤ —Å –∞–∫—Ç—ë—Ä–æ–º...")
                        actor_movie_embeddings = model.encode(actor_movie_descriptions, convert_to_numpy=True, normalize_embeddings=False, batch_size=int(os.getenv('EMBEDDINGS_BATCH_SIZE', '64')))
                        
                        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –¥–æ –∑–∞–ø—Ä–æ—Å–∞
                        query_emb_flat = query_emb[0]
                        distances = []
                        for emb in actor_movie_embeddings:
                            # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ (1 - cosine similarity)
                            dot_product = np.dot(query_emb_flat, emb)
                            norm_query = np.linalg.norm(query_emb_flat)
                            norm_emb = np.linalg.norm(emb)
                            if norm_query > 0 and norm_emb > 0:
                                cosine_sim = dot_product / (norm_query * norm_emb)
                                distance = 1.0 - cosine_sim
                            else:
                                distance = 1.0
                            distances.append(distance)
                        
                        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—é (–±–ª–∏–∂–∞–π—à–∏–µ –ø–µ—Ä–≤—ã–º–∏)
                        sorted_pairs = sorted(zip(valid_indices, distances), key=lambda x: x[1])
                        candidate_indices = [idx for idx, _ in sorted_pairs]
                        candidate_distances = [dist for _, dist in sorted_pairs]
                        
                        logger.info(f"[SEARCH MOVIES] –û—Ç—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–æ {len(candidate_indices)} —Ñ–∏–ª—å–º–æ–≤ —Å –∞–∫—Ç—ë—Ä–æ–º –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É")
                    else:
                        logger.warning(f"[SEARCH MOVIES] –ù–µ –Ω–∞–π–¥–µ–Ω–æ –æ–ø–∏—Å–∞–Ω–∏–π –¥–ª—è —Ñ–∏–ª—å–º–æ–≤ —Å –∞–∫—Ç—ë—Ä–æ–º")
                        # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã —Å –Ω—É–ª–µ–≤—ã–º–∏ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è–º–∏
                        candidate_indices = actor_movie_indices
                        candidate_distances = [0.0] * len(actor_movie_indices)
                else:
                    logger.warning(f"[SEARCH MOVIES] –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∏–ª—å–º–æ–≤ —Å {'–∞–∫—Ç—ë—Ä–æ–º' if is_actor else '—Ä–µ–∂–∏—Å—Å—ë—Ä–æ–º'} '{actor_name_for_search}'")
                    mentioned_actor_en = None
        
        # –ï—Å–ª–∏ –∞–∫—Ç—ë—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –Ω–µ —É–ø–æ–º—è–Ω—É—Ç ‚Äî –æ–±—ã—á–Ω—ã–π FAISS –ø–æ–∏—Å–∫
        if not candidate_indices:
            logger.info(f"[SEARCH MOVIES] –û–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫ (–∞–∫—Ç—ë—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –Ω–µ —É–ø–æ–º—è–Ω—É—Ç)...")
            D, I = index.search(query_emb, k=search_k)
            logger.info(f"[SEARCH MOVIES] –ü–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω, –Ω–∞–π–¥–µ–Ω–æ –∏–Ω–¥–µ–∫—Å–æ–≤: {len(I[0])}")
            candidate_indices = I[0].tolist()
            candidate_distances = [float(D[0][i]) for i in range(len(I[0]))]
            logger.info(f"[SEARCH MOVIES] –û–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫, –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(candidate_indices)}")
        
        # –†–∞–Ω–∂–∏—Ä—É–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        logger.info(f"[SEARCH MOVIES] –®–∞–≥ 6: –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
        results = []
        for i, idx in enumerate(candidate_indices):
            if idx >= len(movies):
                continue
            row = movies.iloc[idx]
            imdb_id_raw = str(row['imdb_id']).strip()
            
            imdb_id_clean = imdb_id_raw.replace('.0', '').replace('.', '').lstrip('t')
            if imdb_id_clean.isdigit():
                imdb_id_clean = f"tt{imdb_id_clean.zfill(7)}"
            else:
                imdb_id_clean = imdb_id_raw
            
            if imdb_id_clean != imdb_id_raw:
                logger.info(f"[SEARCH MOVIES] ID –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω: '{imdb_id_raw}' ‚Üí '{imdb_id_clean}'")
            
            distance = candidate_distances[i]
            
            has_overview = row.get('has_overview', False) if 'has_overview' in row.index else False
            overview_boost = 30 if has_overview else 0  # –±–æ–Ω—É—Å –∑–∞ –Ω–∞–ª–∏—á–∏–µ overview
            
            # –ü–†–ò–û–†–ò–¢–ï–¢ ‚Ññ1: –ë—É—Å—Ç –∑–∞ –ø–æ–ª–Ω–æ–µ –∏–º—è –∞–∫—Ç—ë—Ä–∞/—Ä–µ–∂–∏—Å—Å—ë—Ä–∞ - –°–ê–ú–´–ô –°–ò–õ–¨–ù–´–ô
            # –ë—É—Å—Ç –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –ø–æ–∑–∏—Ü–∏–∏ –∞–∫—Ç—ë—Ä–∞ –≤ —Å–ø–∏—Å–∫–µ:
            # - 1-–π –∞–∫—Ç—ë—Ä = +1000 (–∫–æ–ª–æ—Å—Å–∞–ª—å–Ω—ã–π –±—É—Å—Ç)
            # - 2-5 –∞–∫—Ç—ë—Ä—ã = +600 (–±–æ–ª—å—à–æ–π –±—É—Å—Ç)
            # - 6+ –∞–∫—Ç—ë—Ä—ã = +400 (–æ–±—ã—á–Ω—ã–π –±—É—Å—Ç)
            actor_boost = 0
            director_boost = 0
            if mentioned_actor_en:
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏–º—è –¥–ª—è –ø–æ–∏—Å–∫–∞
                actor_name_for_search = _normalize_text(mentioned_actor_en)
                actors_str = row.get('actors_str', '')
                director_str = row.get('director_str', '')
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤ –∞–∫—Ç—ë—Ä–∞—Ö (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç ‚Ññ1)
                if 'actors_str' in row.index and pd.notna(actors_str):
                    actors_normalized = _normalize_text(str(actors_str))
                    if actor_name_for_search in actors_normalized:
                        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–∑–∏—Ü–∏—é –∞–∫—Ç—ë—Ä–∞ –≤ —Å–ø–∏—Å–∫–µ (–ø—Ä–∏–º–µ—Ä–Ω–æ, –ø–æ –ø–æ—Ä—è–¥–∫—É —Å–ª–æ–≤)
                        actors_list = actors_normalized.split(',')
                        position = 0
                        for idx, actor in enumerate(actors_list):
                            if actor_name_for_search in actor:
                                position = idx + 1
                                break
                        
                        if position == 1:
                            actor_boost = 1000  # 1-–π –∞–∫—Ç—ë—Ä
                        elif position <= 5:
                            actor_boost = 600   # 2-5 –∞–∫—Ç—ë—Ä—ã
                        else:
                            actor_boost = 400   # 6+ –∞–∫—Ç—ë—Ä—ã
                        logger.info(f"[SEARCH MOVIES] –ê–∫—Ç—ë—Ä '{mentioned_actor_en}' –Ω–∞–π–¥–µ–Ω –≤ –ø–æ–∑–∏—Ü–∏–∏ {position} ‚Üí +{actor_boost} –¥–ª—è {imdb_id_clean}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤ —Ä–µ–∂–∏—Å—Å—ë—Ä–∞—Ö (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç ‚Ññ2, —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –∞–∫—Ç—ë—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω)
                if actor_boost == 0 and 'director_str' in row.index and pd.notna(director_str):
                    director_normalized = _normalize_text(str(director_str))
                    if actor_name_for_search in director_normalized:
                        director_boost = 400  # –†–µ–∂–∏—Å—Å—ë—Ä –≤—Å–µ–≥–¥–∞ +400
                        logger.info(f"[SEARCH MOVIES] –†–µ–∂–∏—Å—Å—ë—Ä '{mentioned_actor_en}' –Ω–∞–π–¥–µ–Ω ‚Üí +{director_boost} –¥–ª—è {imdb_id_clean}")
            
            # –ü–†–ò–û–†–ò–¢–ï–¢ ‚Ññ2: Keyword-–º–∞—Ç—á–∏–Ω–≥ –ø–æ overview (√ó25 –∑–∞ –∫–∞–∂–¥–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ)
            overview_keyword_matches = 0
            if keywords and 'overview' in row.index:
                overview_text_normalized = _normalize_text(row.get('overview', ''))
                overview_keyword_matches = sum(1 for word in keywords if word in overview_text_normalized)
            
            # –ü–†–ò–û–†–ò–¢–ï–¢ ‚Ññ3: –ë—É—Å—Ç –∑–∞ –∂–∞–Ω—Ä (–µ—Å–ª–∏ –∂–∞–Ω—Ä —É–ø–æ–º—è–Ω—É—Ç –≤ –∑–∞–ø—Ä–æ—Å–µ –∏ –µ—Å—Ç—å –≤ —Ñ–∏–ª—å–º–µ)
            genre_boost = 0
            if detected_genres and 'genres_str' in row.index:
                genres_str_normalized = _normalize_text(row.get('genres_str', ''))
                for genre in detected_genres:
                    if genre in genres_str_normalized:
                        genre_boost += 100  # –°–∏–ª—å–Ω—ã–π –±—É—Å—Ç –∑–∞ –∫–∞–∂–¥—ã–π —Å–æ–≤–ø–∞–¥–∞—é—â–∏–π –∂–∞–Ω—Ä
                        logger.info(f"[SEARCH MOVIES] –ñ–∞–Ω—Ä '{genre}' –Ω–∞–π–¥–µ–Ω ‚Üí +100 –¥–ª—è {imdb_id_clean}")
            
            # –ü–†–ò–û–†–ò–¢–ï–¢ ‚Ññ4: Keyword-–º–∞—Ç—á–∏–Ω–≥ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é (–Ω–µ–±–æ–ª—å—à–æ–π –±—É—Å—Ç)
            title_keyword_matches = 0
            title_boost = 0
            if keywords and 'title' in row.index:
                title_text_normalized = _normalize_text(row.get('title', ''))
                title_keyword_matches = sum(1 for word in keywords if word in title_text_normalized)
                # –ù–µ–±–æ–ª—å—à–æ–π –±—É—Å—Ç –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ (–Ω–µ –æ—á–µ–Ω—å —Å–∏–ª—å–Ω—ã–π)
                if title_keyword_matches > 0:
                    title_boost = 5 * title_keyword_matches  # –ù–µ–±–æ–ª—å—à–æ–π –±—É—Å—Ç –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏
            
            # –ë—É—Å—Ç –ø–æ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ (vote_count)
            popularity_boost = 0
            if 'vote_count' in row.index:
                vote_count = row.get('vote_count', 0)
                if pd.notna(vote_count) and isinstance(vote_count, (int, float)):
                    # –õ–∏–Ω–µ–π–Ω—ã–π –±—É—Å—Ç: min(vote_count / 1000, 150) - –º–∞–∫—Å–∏–º—É–º +150
                    popularity_boost = min(float(vote_count) / 1000.0, 150.0)
            
            # –ë—É—Å—Ç –∑–∞ —Å–≤–µ–∂–µ—Å—Ç—å (–≥–æ–¥ > 2000)
            freshness_boost = 0
            year = row.get('year')
            if pd.notna(year) and isinstance(year, (int, float)):
                year_int = int(year)
                if year_int > 2000:
                    freshness_boost = 25  # –±–æ–Ω—É—Å –∑–∞ —Å–≤–µ–∂–µ—Å—Ç—å
            
            # –ë–∞–∑–æ–≤—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π score
            # –ü—Ä–∏ –≤—ã—Å–æ–∫–æ–º FUZZINESS_LEVEL –¥–µ–ª–∞–µ–º —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –º—è–≥—á–µ (–º–µ–Ω—å—à–µ —à—Ç—Ä–∞—Ñ –∑–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ)
            # FUZZINESS_LEVEL –≤–ª–∏—è–µ—Ç –Ω–∞ –≤–µ—Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è:
            # 0 = —Å—Ç—Ä–æ–≥–∏–π (distance –∏–º–µ–µ—Ç –ø–æ–ª–Ω—ã–π –≤–µ—Å)
            # 50 = —Å—Ä–µ–¥–Ω–∏–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
            # 100 = –º—è–≥–∫–∏–π (distance –∏–º–µ–µ—Ç –º–µ–Ω—å—à–∏–π –≤–µ—Å, –±–æ–ª—å—à–µ —É—á–∏—Ç—ã–≤–∞—é—Ç—Å—è keyword-–º–∞—Ç—á–∏)
            distance_weight = 1.0 - (FUZZINESS_LEVEL / 200.0)  # –û—Ç 1.0 –¥–æ 0.5
            base_score = 1.0 - (distance * distance_weight)
            
            # –ò—Ç–æ–≥–æ–≤—ã–π score —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏:
            # –ü–†–ò–û–†–ò–¢–ï–¢ ‚Ññ1: actor_boost (+400) - –°–ê–ú–´–ô –°–ò–õ–¨–ù–´–ô
            # –ü–†–ò–û–†–ò–¢–ï–¢ ‚Ññ2: overview_keyword_matches (√ó25, –Ω–æ –ø—Ä–∏ –≤—ã—Å–æ–∫–æ–º fuzziness –º–æ–∂–µ—Ç –±—ã—Ç—å –±–æ–ª—å—à–µ)
            # –ü–†–ò–û–†–ò–¢–ï–¢ ‚Ññ3: genre_boost (+100 –∑–∞ –∂–∞–Ω—Ä)
            # –ü–†–ò–û–†–ò–¢–ï–¢ ‚Ññ4: title_boost (+5 –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏)
            # –ü—Ä–∏ –≤—ã—Å–æ–∫–æ–º FUZZINESS_LEVEL —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤–µ—Å keyword-–º–∞—Ç—á–∏–Ω–≥–∞ (—Å–∏–Ω–æ–Ω–∏–º—ã –≤–∞–∂–Ω–µ–µ)
            keyword_multiplier = 25.0 + (FUZZINESS_LEVEL / 100.0) * 15.0  # –û—Ç 25 –¥–æ 40
            score = base_score + actor_boost + director_boost + (overview_keyword_matches * keyword_multiplier) + genre_boost + title_boost + overview_boost + freshness_boost + popularity_boost
            
            results.append({
                'imdb_id': imdb_id_clean,
                'title': row['title'],
                'year': row['year'] if pd.notna(row['year']) else None,
                'description': row['description'][:500] if 'description' in row.index else '',
                'distance': distance,
                'has_overview': has_overview,
                'overview_keyword_matches': overview_keyword_matches,
                'overview_boost': overview_boost,
                'freshness_boost': freshness_boost,
                'popularity_boost': popularity_boost,
                'actor_boost': actor_boost,
                'genre_boost': genre_boost,
                'title_boost': title_boost,
                'score': score
            })
        
        results.sort(key=lambda x: x['score'], reverse=True)
        results = results[:top_k]
        
        logger.info(f"[SEARCH MOVIES] –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω—ã, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º {len(results)} —Ñ–∏–ª—å–º–æ–≤")
        if results:
            logger.info(f"[SEARCH MOVIES] –¢–æ–ø-3: {[(r['title'], r['actor_boost'], r['overview_keyword_matches'], r['genre_boost'], r['title_boost'], r['score']) for r in results[:3]]}")
        return results
    except Exception as e:
        logger.error(f"[SEARCH MOVIES] –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Ñ–∏–ª—å–º–æ–≤: {e}", exc_info=True)
        return []